{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"RAG_Techniques/test.ipynb\"\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "nbformat.write(nb, notebook_path)\n",
    "\n",
    "print(\"Fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8076,
     "status": "ok",
     "timestamp": 1770212535813,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "_3NdtP7RjOqs",
    "outputId": "8420e8cd-7ac8-4d1f-a02f-29bee209de80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: pypdf, ollama, langchain-text-splitters, langchain-ollama, langchain-huggingface, langchain-community, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2 langchain-community-0.4.1 langchain-huggingface-1.2.0 langchain-ollama-1.0.1 langchain-text-splitters-1.1.0 ollama-0.6.1 pypdf-6.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters  pypdf  langchain-huggingface sentence-transformers faiss-cpu ollama langchain-ollama   --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30945,
     "status": "ok",
     "timestamp": 1770212566768,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "AWplurkTjcag",
    "outputId": "338dfacd-c6e9-4497-f086-75c90dbc75eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 第一個 Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# 注意：這裡刻意不寫 torch，直接用 Colab 內建的，就不會打架也不會慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2449,
     "status": "ok",
     "timestamp": 1770213825467,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "0VjS0XQKjqg7",
    "outputId": "6aa65592-3bc8-4569-8e88-5cbaf449f238"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Ollama 模型路徑已設定為: /content/drive/MyDrive/RAG/ollama_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. 掛載 Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. 設定模型儲存路徑 (建議在 Drive 建立一個專門的資料夾)\n",
    "# 這裡設定為 MyDrive 下的 RAG_project/ollama_models 資料夾\n",
    "my_model_folder = '/content/drive/MyDrive/RAG/ollama_models'\n",
    "\n",
    "# 如果資料夾不存在，建立它\n",
    "if not os.path.exists(my_model_folder):\n",
    "    os.makedirs(my_model_folder)\n",
    "\n",
    "# 3. 【關鍵】設定環境變數，讓 Ollama 知道去哪裡找模型\n",
    "os.environ['OLLAMA_MODELS'] = my_model_folder\n",
    "\n",
    "print(f\"Ollama 模型路徑已設定為: {os.environ['OLLAMA_MODELS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41315,
     "status": "ok",
     "timestamp": 1770213868041,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "PT9_GbqOrrQw",
    "outputId": "c95fc617-fb98-4c84-974a-f27f4d27f467"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在安裝相依套件 zstd...\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "zstd is already the newest version (1.4.8+dfsg-3build1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
      "正在安裝 Ollama...\n",
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "正在啟動 Ollama 服務...\n",
      "等待服務啟動中 (約 10 秒)...\n",
      "✅ 成功：Ollama 服務已在背景執行！\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# --- 2. 修正並安裝 Ollama ---\n",
    "print(\"正在安裝相依套件 zstd...\")\n",
    "!sudo apt-get install -y zstd  # <--- 新增這一行解決你的錯誤\n",
    "\n",
    "print(\"正在安裝 Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# --- 3. 啟動 Ollama 服務 ---\n",
    "print(\"正在啟動 Ollama 服務...\")\n",
    "# 使用完整路徑以防萬一 (通常是 /usr/local/bin/ollama)\n",
    "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# 等待服務啟動\n",
    "print(\"等待服務啟動中 (約 10 秒)...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# --- 4. 測試連線 ---\n",
    "try:\n",
    "    # 檢查服務是否活著\n",
    "    check = subprocess.run([\"curl\", \"-s\", \"http://localhost:11434\"], capture_output=True, text=True)\n",
    "    if \"Ollama is running\" in check.stdout:\n",
    "        print(\"✅ 成功：Ollama 服務已在背景執行！\")\n",
    "    else:\n",
    "        print(\"⚠️ 警告：服務似乎未回應，請檢查日誌。\")\n",
    "except Exception as e:\n",
    "    print(f\"檢查連線時發生錯誤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKx6Vt_Kjux1"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List,Dict,Any,Tuple\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from pydantic import Field,BaseModel\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import faiss\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.retrievers import BaseRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "ad9e8100699c4c00a248026624649296",
      "ba645786a45b46d7a926cd623a3b286c",
      "df8baf30675f445bbc9970af4b1734b7",
      "a95dc18baa38484bb6554d343513e09c",
      "1b4c68be4e194c9cbb273ab425520126",
      "01554fa68a844ad080de42e2e916ed51",
      "998f7c909b7c4a52aad6b401457af5bf",
      "ec3695fd492d45838091bbead8f7e838",
      "302cbfbdbcce432b9975ebdae90707a4",
      "570ea521061043f8b31e8fb862132626",
      "06eb1da3ea514c6ba3dda724b1d9314b",
      "2720596515d0425d9723dcec295889c6",
      "92c1660dffdf4beaaf5ec6f20da538af",
      "104e92d0908a488a845a2b51bcefea2e",
      "d3859ac9a7fd4ae29b88d59429930e27",
      "2606bf40248b48be9a82f82f8954a645",
      "2887747323c54cc3b953c7646cc6a1f6",
      "1d6c4ddbf86b411b8e5699bdd2be925e",
      "069dbb52c4534e0cb82264504fc7b030",
      "157df928b3364f1e8297d8f92edba9fc",
      "933df723d91742a9a75416f0c0af0890",
      "b3f9c31cd69d444aa7555d88f7d896b0",
      "6830796f5f044eba9590aa7b07b5690d",
      "b786d793406341659bb4a9007ff290dd",
      "9cbc556968e24244bf24b1c89b733f17",
      "9aadea7eb5ae4ac9b6856d51195bbd25",
      "9f8cb99555df40208e2c751a39df8e3a",
      "196d043c34484c0595a1a68256276983",
      "3fb2a5ec1d6d4ab8afa49bde6cc2bfeb",
      "773641f32feb4439aa1cb9084d977c46",
      "f8bada97be704fde8a15873d902d9b54",
      "9f2d8415b78745acacae3e9381f895ff",
      "18aaa02ddbae4230a3662ca4c8f6c087",
      "987b9e93278d449bb1d4ca6860d42705",
      "ad82dadc479f4ea2ba94400a82e420e3",
      "921836be5ced4886bffa27b0c944a6f0",
      "94e82bc8da9a40ea890d12a2e8847d69",
      "a46b2a3e58fb4b6eb54003e81a3d0ad1",
      "6428af83f80f41a2a764f8b1ca42a5fc",
      "83ea51bfa6d748438c6b369a7dae79ad",
      "56f770fc67934c6bbd170c9f5d2653ae",
      "e40e60445437419186cee29eed41e75e",
      "e80f728ce82143c6a98e0faabfe9c0d5",
      "2b3fb8ccf9484f27823d7c983617f48c",
      "3f58a28717c3414386faa99906a6cda6",
      "7ad77750caf14f5b99a2a7417248060a",
      "2fad320a8bc04d10b48ac3597bda48f4",
      "3622d1c6480449c5a68b12fd1bb2d796",
      "1e3fe4c2549b43b7b720754908c59c86",
      "8be8c2eae5d5418eb829b847b04c89d2",
      "604ec23521ba43818a4f239f8997ee01",
      "ad0d88ecc3af4d87b2d597e74d7812e6",
      "501cbb50093b4631b530485d5437cb25",
      "1112cc1f531b44629edbbd14907cc252",
      "918b8c0f4a1c43b3946d512d28b39d1f",
      "2f919524225e4b84920f1cbad96bf2bc",
      "faa718082cd94580b587770cbbb91b35",
      "479ecf762d214074bb58fb06d05195b6",
      "0a1ad035d5c840a38a0f676aac556d7b",
      "d9604380768545dfaf4692bcb717a888",
      "9f0a98b8a8b742f8bd6ef8160b3b20ec",
      "5f59a8a62e394a349e330cfcdd5525f1",
      "5646393dd9a641c0aca48566bba768f3",
      "f018b65732f64264a07d6deb132ca6d9",
      "1543e1453a704393881cf2ad22cb3863",
      "a63d4221f6884e12bfd6b43388c80be1",
      "c8dba95ca5594cc9b28c73525697ccf0",
      "49d7258bdfbe4a948071867d937277a3",
      "5e0700abc50e4db9a1d75d365b19414a",
      "0dd08491a587447dad883fcc1b19fd50",
      "595edae0dfee47f2858590bc6245f9a4",
      "53874dd8e60248199a0508a53d662df6",
      "77c462e22fff46c78ecda3cf8a6d54da",
      "dad530a648f042cb81de66ba46d43cbf",
      "207cd592dd43427cac559c5ce9b97d84",
      "1aae994883e44913929103299b43ef29",
      "e69cf33ae90c4aacaa97b45861212cb3",
      "b9c6d12eab8a40da87c30fb560f28e70",
      "11f39aeb66d749609746c430796cf0a8",
      "5647032592044e2095a53db2491ff8b1",
      "1999cd46fcab4e98b425f48b76902983",
      "6a596724ca5e48a99183b60b8407e3a7",
      "01225179fef446b78b982a8d290d1161",
      "1dd7480c727f45029766465e2d864d6c",
      "5beac326410044778a04a65e3c9093ad",
      "7c5a2cc4d913468b81521f5b3ba0c2c1",
      "d15b012bc6474e0a8bcc9411ebe0c8d1",
      "8251acf251464ba6b043c9768727f1f6",
      "c45a0d20f0054712be80b89d1ad81b09",
      "366562a8fc144b6d9b877dc9679c94ba",
      "57e650025d754e469a0577e70847fd35",
      "b23d787ad84d4c359386902a3323d6cb",
      "68e34de327864cc88299b822a61e126f",
      "5389f28d99a047a5826f9e286a3ee5d0",
      "236c4f4d4a5746c985e857a9ebe6d68b",
      "063a812e49944d50acd410170a56d756",
      "cd24f62553b74d1fb99a18f1c9da66c9",
      "59981b6497fe4de0a82d88c01fc572d2",
      "15412ae6bc7347d68ebd07be44ab7c65",
      "ff693483b7964f0286eb38e7f3a39ec7",
      "700ed43866124df0ae56883e890e6d82",
      "50ade9286f8f4ebfa4b250ef3452610a",
      "f40d0620231842f29a8fc1ee5b64d6cd",
      "55be33b6289a4e8f8e8091b6e14b1d17",
      "f9753faa129c488ebace0b7d3bb3b538",
      "d3ffd55f395d4340939fcaa3e35a15be",
      "656c22e41e4943318875db9365aa0fc7",
      "4051cf2b9a8b471dad92701e24e998a2",
      "01c0f58ccc044f8cb34fc5f2e5917425",
      "045bdc36c74a4b60b07bc36122a895ea",
      "6cd8eb2e883146f69758b1867e5d5991",
      "bf0ba8b17afc44658d60bb3e0358e114",
      "2ed2991bdd294fa8aa97fc41b7db4948",
      "131fd785ab52425b8cd897ac37eff335",
      "773a303cca3549cd88e0b82b9c7d7ca9",
      "73a118b4cde64174bd86e90cfebe63c8",
      "307f11a982cb47dd8e87dc1c26065d1e",
      "3f31a1e0f67642e29bc5b87e880d96b1",
      "7434444922504257a38c0aba4dc7c8d8",
      "ee49b4d177d84766872a8022cd228328",
      "2c4cbc8eca5f42989dfea207c126dea8"
     ]
    },
    "executionInfo": {
     "elapsed": 30656,
     "status": "ok",
     "timestamp": 1770212707221,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "-e4lrP0zjv19",
    "outputId": "94591fce-cd8a-4076-c105-b26b0753333a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9e8100699c4c00a248026624649296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2720596515d0425d9723dcec295889c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6830796f5f044eba9590aa7b07b5690d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "987b9e93278d449bb1d4ca6860d42705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f58a28717c3414386faa99906a6cda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f919524225e4b84920f1cbad96bf2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8dba95ca5594cc9b28c73525697ccf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c6d12eab8a40da87c30fb560f28e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c45a0d20f0054712be80b89d1ad81b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff693483b7964f0286eb38e7f3a39ec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd8eb2e883146f69758b1867e5d5991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 102,
     "status": "ok",
     "timestamp": 1770212707326,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "7XRzTvLzmNfk"
   },
   "outputs": [],
   "source": [
    "path=r\"/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf\"\n",
    "chunk_size=512\n",
    "chunk_overlap=64\n",
    "query=\"What the CRAG proposed to improve \"\n",
    "llm=ChatOllama(model=\"qwen3:4b-instruct-2507-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 15143,
     "status": "ok",
     "timestamp": 1770212786758,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "B3Us01M3mW55"
   },
   "outputs": [],
   "source": [
    "def replace_tab_with_space(docs):\n",
    "    for doc in docs:\n",
    "        doc.page_content=doc.page_content.replace(\"\\t\",\" \")\n",
    "    return docs\n",
    "\n",
    "def encode_pdf(path,chunk_size,chunk_overlap):\n",
    "\n",
    "    loader=PyPDFLoader(path)\n",
    "    documents=loader.load()\n",
    "\n",
    "    splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    chunks=splitter.split_documents(documents)\n",
    "\n",
    "    cleaned_chunks=replace_tab_with_space(chunks)\n",
    "\n",
    "    vectorstore=FAISS.from_documents(cleaned_chunks,embedding_model)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "vectorstore=encode_pdf(path,chunk_size,chunk_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HSGCBCDPvdiL"
   },
   "source": [
    "##Method 1: LLM based function to rerank the retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1770209197699,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "h3yT6_ZWmYrf"
   },
   "outputs": [],
   "source": [
    "class RatingScore(BaseModel):\n",
    "    relevace_score:float = Field(...,description=\"The relevance score of documents to a query\")\n",
    "\n",
    "def rerank_documents(query:str, docs:List[Document], top_n:int=3 )->List[Document]:\n",
    "    template=\"\"\"\n",
    "    on the scale of 1-10,rate the relevace of following document to the query.\n",
    "    consider the specific context and intent of query,not just keyword matches.\n",
    "    Query:{query}\n",
    "    Document:{doc}\n",
    "    relevace score:\n",
    "    \"\"\"\n",
    "    prompt=PromptTemplate.from_template(\n",
    "        template=template,\n",
    "        )\n",
    "\n",
    "    llm_chain=prompt | llm.with_structured_output(RatingScore)\n",
    "\n",
    "    scored_docs=[]\n",
    "    for doc in docs:\n",
    "        input_data={\"query\":query,\"doc\":doc.page_content}\n",
    "        score=llm_chain.invoke(input_data).relevace_score\n",
    "\n",
    "        try:\n",
    "            score=float(score)\n",
    "        except ValueError:\n",
    "            score=0\n",
    "        scored_docs.append((doc,score))\n",
    "\n",
    "    reranked_docs=sorted(scored_docs,key= lambda x : x[1], reverse=True)\n",
    "    return [doc for doc,_ in reranked_docs[:top_n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1770209199426,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "eYdvTyMGouKS",
    "outputId": "de078686-5e24-44e9-c9f5-4b0b2d42dee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='283e9bea-12dd-4cc4-b0fd-95763ef21175', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='improvement of our method.\\nSelf-CRAG: To demonstrate that our plug-and-\\nplay approach can be utilized in other concurrent\\nstudies, we specifically designed to insert our\\nCRAG into the Self-RAG (Asai et al., 2024)\\nframework and named it Self-CRAG. Self-RAG'), Document(id='6483e03b-d484-459c-afa2-210ff95957a4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='Generation ( CRAG ) is proposed to self-correct\\nthe results of retriever and improve the utilization\\nof documents for augmenting generation. A\\nlightweight retrieval evaluator is designed to\\nassess the overall quality of retrieved documents\\nfor a query. This serves as a crucial component\\nin RAG, contributing to informative generation\\nby reviewing and evaluating the relevance\\nand reliability of the retrieved documents. A\\nconfidence degree is quantified based on which'), Document(id='d0bcf298-6d03-4d94-bde4-fedd552a84a6', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='judgment, both types of processed knowledge in\\nCorrect and Incorrect are combined to comple-\\nment each other. Implementing such a moderating\\nand soft strategy can significantly contribute to\\nstrengthening the robustness and resilience of the\\nsystem, fostering a more adaptable framework for\\noptimal performance.\\nDiscussion Preliminary experiments of employ-\\ning only the Correct and Incorrect actions show\\nthat the efficacy of CRAG was easily affected by\\nthe accuracy of the retrieval evaluator. The reason'), Document(id='898e7ce6-0bed-4ab0-a759-547d5ec50272', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='where the retriever returns inaccurate results and,\\nto the best of our knowledge, makes the first\\nattempt to design corrective strategies for RAG to\\nimprove its robustness. 2) A plug-and-play method\\nnamed CRAG is proposed to improve the ability of\\nautomatic self-correction and efficient utilization\\nof retrieved documents. 3) Experimental results\\nextensively demonstrate CRAG ’s adaptability to\\nRAG-based approaches and its generalizability\\nacross short- and long-form generation tasks.\\n2 Related Work'), Document(id='0d05b521-3097-4ebc-b0dc-d5a83bcf8600', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='retrieval evaluator is to estimate and trigger three\\nknowledge retrieval actions discriminately. With\\nthe further leverage of web search and optimized\\nknowledge utilization, CRAG has significantly\\nimproved the ability of automatic self-correction\\nand efficient utilization of retrieved documents.\\nExperiments extensively demonstrate its adaptabil-\\nity to RAG-based approaches as well as general-\\nizability across short- and long-form generation\\ntasks. While we primarily proposed to improve the'), Document(id='f3216611-df47-4496-ab12-763ac99b3411', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='2023a), and Arc-Challenge (Bhakthavatsalam et al.,\\n2021) show that CRAG can significantly improve\\nthe performance of standard RAG and state-of-the-\\nart Self-RAG, demonstrating its generalizability\\nacross both short- and long-form generation tasks.\\nTo facilitate others to reproduce our results, we will\\npublish all source code later.\\nIn summary, our contributions in this paper are\\nthree-fold: 1) This paper studies the scenarios\\nwhere the retriever returns inaccurate results and,'), Document(id='b09671a6-36a9-4ab1-9ac4-f15382ad2da1', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='The same metrics are used because our proposed\\nmethod is comparable to previous studies, since\\nwe used the same retrieval results as previous\\nwork. The difference lies in that our motivation\\nis to improve the retrieval quality by correcting\\nthe retrieval results that the system judges to\\nbe of low quality. This can be analogous to\\nRAG’s augmentation to standalone parameterized\\nlanguage models and we further augment RAG\\nwith corrective strategies.\\n5.2 Baselines\\nWe primarily compared CRAG with both ap-'), Document(id='719f6403-cb73-473d-8816-2bb082e014b4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='Algorithm 1: CRAG Inference\\nRequire :E (Retrieval Evaluator), W (Query Rewriter), G (Generator)\\nInput : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents)\\nOutput : y (Generated response)\\n1 scorei = E evaluates the relevance of each pair (x, di), di ∈ D\\n2 Confidence = Calculate and give a final judgment based on {score1, score2, ...scorek}\\n// Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]\\n3 if Confidence == [CORRECT] then\\n4 Internal_Knowledge = Knowledge_Refine(x, D)'), Document(id='0c345097-5883-43e4-9cd1-7c1c1e44d567', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='as reference knowledge both during retrieval and\\nutilization. But a considerable portion of the text\\nwithin these retrieved documents is often non-\\nessential for generation, which should not have\\nbeen equally referred to and involved in RAG.\\nOn account of the above issues, this paper\\nparticularly studies the scenarios where\\nthe retriever returns inaccurate results. A\\nmethod named Corrective Retrieval-Augmented\\nGeneration ( CRAG ) is proposed to self-correct'), Document(id='af4acf3e-6afd-4cd2-afb3-191e67703d52', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='ography), and closed-set tasks (PubHealth, Arc-\\nChallenge). These results verified the consistent\\neffectiveness ofCRAG . Its versatility across a spec-\\ntrum of tasks underscores its robust capabilities and\\ngeneralizability across diverse scenarios.\\nThird, the proposed method exhibited greater\\nflexibility in replacing the underlying LLM gen-\\nerator. It can be seen that CRAG still showed\\ncompetitive performance when the underlying\\nLLMs was changed from SelfRAG-LLaMA2-7b'), Document(id='4b543012-a3a5-48b9-834e-60647257e2d3', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='LLMs was changed from SelfRAG-LLaMA2-7b\\nto LLaMA2-hf-7b, while the performance of Self-\\nRAG dropped significantly, even underperforming\\nthe standard RAG on several benchmarks. The\\nreason for these results is that Self-RAG needs to be\\ninstruction-tuned using human or LLM annotated\\ndata to learn to output special critic tokens as\\nneeded, while this ability is not learned in common\\nLLMs. CRAG does not have any requirements\\nfor this ability. As you can imagine, when more'), Document(id='ce38302b-df94-4196-a39d-40fbebd606d2', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='Table 11: Ablation study for removing only a single\\naction on the PopQA dataset in terms of accuracy.\\nLLaMA2-hf-7b SelfRAG-LLaMA2-7b\\nCRAG 54.9 59.8\\nonlyCorrect 52.4 56.7\\nonlyIncorrect 47.0 48.5\\nonlyAmbiguous 52.7 58.0\\nSelf-CRAG 49.0 61.8\\nonlyCorrect 48.6 57.2\\nonlyIncorrect 40.8 53.3\\nonlyAmbiguous 44.9 59.8\\nis an advanced RAG approach that introduces a\\ncritic model to decide whether to retrieve and which\\nretrieved document to be referred for generation. It\\nmeets our demand for deciding which action to be'), Document(id='e345dd70-28f4-4d96-a2a3-928ec90b6b6b', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='same knowledge refinement method as Section 4.4\\nto derive the relevant web knowledge, namely\\nexternal knowledge.\\n5 Experiments\\nWe conducted experiments to extensively demon-\\nstrate CRAG ’s adaptability to RAG-based ap-\\nproaches and its generalizability across both short-\\nand long-form generation tasks.\\n5.1 Tasks, Datasets and Metrics\\nCRAG was evaluated on four datasets, including\\nPopQA (Mallen et al., 2023) ( short-form gener-\\nation), Biography (Min et al., 2023) ( long-form'), Document(id='56b488f7-4f2a-40bb-b599-c2bacac70222', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='tasks. While we primarily proposed to improve the\\nRAG framework from a corrective perspective and\\nCRAG can be seamlessly coupled with various\\nRAG-based approaches, fine-tuning an external\\nretrieval evaluator is inevitable. How to eliminate\\nthis external evaluator and equip LLMs with better\\nretrieval evaluation capabilities will be our future\\nwork.\\nReferences\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng'), Document(id='0e0917f2-babe-4ec0-b26b-889338716dfb', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-10-08T01:19:39+00:00', 'author': '', 'keywords': '', 'moddate': '2024-10-08T01:19:39+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='optimizing the extraction of key insights and\\nminimizing the inclusion of non-essential elements,\\nthereby enhancing the utilization of retrieved data.\\nCRAG is plug-and-play and experimentally\\nimplemented into RAG (Lewis et al., 2020) and\\nSelf-RAG (Asai et al., 2024) for demonstrating its\\nadaptability to RAG-based approaches. Results on\\nfour datasets of PopQA (Mallen et al., 2023), Biog-\\nraphy (Min et al., 2023), Pub Health (Zhang et al.,\\n2023a), and Arc-Challenge (Bhakthavatsalam et al.,')]\n"
     ]
    }
   ],
   "source": [
    "initial_docs = vectorstore.similarity_search(query, k=15)\n",
    "print(initial_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1770209812116,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "08-Rofxtt2G4",
    "outputId": "c6be61bb-1313-444a-f9a2-c18942ee3973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root       13175  0.1  0.2 1785252 33436 ?       Sl   12:56   0:00 ollama serve\n",
      "root       13303  0.0  0.0   7376  3484 ?        S    12:56   0:00 /bin/bash -c ps aux | grep ollama\n",
      "root       13305  0.0  0.0   6484  2404 ?        S    12:56   0:00 grep ollama\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 196006,
     "status": "ok",
     "timestamp": 1770210009963,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "5Hciyk87pQt-"
   },
   "outputs": [],
   "source": [
    "reranked_docs = rerank_documents(query, initial_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1770210020762,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "RFBpkhLRsO1G",
    "outputId": "6fbbb803-e84c-4f5a-8626-8582e3a0c7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top initial documents:\n",
      "\n",
      "Document 1:\n",
      "improvement of our method.\n",
      "Self-CRAG: To demonstrate that our plug-and-\n",
      "play approach can be utilized in other concurrent\n",
      "studies, we specifically designed to insert our\n",
      "CRAG into the Self-RAG (Asai et al., 2024)\n",
      "framework and named it Self-CRAG. Self-RAG\n",
      "\n",
      "Document 2:\n",
      "Generation ( CRAG ) is proposed to self-correct\n",
      "the results of retriever and improve the utilization\n",
      "of documents for augmenting generation. A\n",
      "lightweight retrieval evaluator is designed to\n",
      "assess the overall quality of retrieved documents\n",
      "for a query. This serves as a crucial component\n",
      "in RAG, contributing to informative generation\n",
      "by reviewing and evaluating the relevance\n",
      "and reliability of the retrieved documents. A\n",
      "confidence degree is quantified based on which\n",
      "Query: What the CRAG proposed to improve \n",
      "\n",
      "Top reranked documents:\n",
      "\n",
      "Document 1:\n",
      "where the retriever returns inaccurate results and,\n",
      "to the best of our knowledge, makes the first\n",
      "attempt to design corrective strategies for RAG to\n",
      "improve its robustness. 2) A plug-and-play method\n",
      "named CRAG is proposed to improve the ability of\n",
      "automatic self-correction and efficient utilization\n",
      "of retrieved documents. 3) Experimental results\n",
      "extensively demonstrate CRAG ’s adaptability to\n",
      "RAG-based approaches and its generalizability\n",
      "across short- and long-form generation tasks.\n",
      "2 Related Work\n",
      "\n",
      "Document 2:\n",
      "Generation ( CRAG ) is proposed to self-correct\n",
      "the results of retriever and improve the utilization\n",
      "of documents for augmenting generation. A\n",
      "lightweight retrieval evaluator is designed to\n",
      "assess the overall quality of retrieved documents\n",
      "for a query. This serves as a crucial component\n",
      "in RAG, contributing to informative generation\n",
      "by reviewing and evaluating the relevance\n",
      "and reliability of the retrieved documents. A\n",
      "confidence degree is quantified based on which\n"
     ]
    }
   ],
   "source": [
    "print(\"Top initial documents:\")\n",
    "for i, doc in enumerate(initial_docs[:2]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)  # Print first 200 characters of each document\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Top reranked documents:\")\n",
    "for i, doc in enumerate(reranked_docs[:2]):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(doc.page_content)  # Print first 200 characters of each document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "903G0zpGu5Xg"
   },
   "source": [
    "##Method 2: Cross Encoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "5630b31acd7f4723a109bf3dfe78d91a",
      "66f6a91d4bb64dee84e987faa2502929",
      "3b33cc0a97bf4b229a7841dd524ca5a5",
      "df952458f93144f697f5714644340c5c",
      "5fdf7af48673426b98259c955ce5b8bc",
      "99740460a93f4673a70181a6db5457be",
      "8bdfcf1e195442f4ae88338b3b120451",
      "1d780b7548c44ee298bbb263c8e669b7",
      "afbb464e262f406c8e38da2aa9e1583d",
      "9212965b797949b6b2464b040228205f",
      "e4a4a61776784b3e807bf7c664810ab6",
      "7a411c674a6345af963ac668ca1faa22",
      "62f73bdbccb447d7bb25f7d73a6d8dbf",
      "ee86db8c3ee946d98a1420a5f1731d68",
      "49cc6c42b5064c42bc2ac6b5393f7efb",
      "abc2457b6ccf431d8a69afd7c2e88504",
      "f62253fc11364c71b0a2d3c4b744be49",
      "7ac741444581413a851cea5913803024",
      "bd2fb71c462a4d9e8b065a8607fca9ed",
      "548705bc38f24ce8adfa8b9956b58168",
      "ac928eec38bd48a0ad31a4c410aa11e9",
      "4bf8f664c0bb4c289f983f94ca6b16f8",
      "70437e707f7145ada0c0cc43c791b8ef",
      "6bd0552dd4b0455c9f12c1e205a2132b",
      "8a67a9308fca40168bb3af34795bc266",
      "2a5a4cb73df14d1bbda88a0b14842700",
      "8750cecce46c43a3a978c33f644f35f1",
      "0b3ff65d1bd842ea9dc1f81ad590411e",
      "aa3aebd48ba042b8bfe5682a773b2c9c",
      "523a60a572e1498fa7dd925cc622a000",
      "075654b8a7104963af5e62154157bc95",
      "25b5a3b271db40ca97b3851aafab459b",
      "0eec05649241479fad837115206bc4d9",
      "0d693cce64c24c069028fd5f15987712",
      "d3a464c073844756a715d8bb14e61480",
      "7f95999dd427484b81e9ce3114dbf878",
      "169814cfb7df467a81c873ebcc7f9f35",
      "aa5b2221d7844455bbd4cdbe863234bb",
      "52a6fbedc19642e8811a1b240e57d904",
      "70ec9cc08380423b9a1dc7314272ca25",
      "1f5b320356ad43d8af51db9c33d65c3a",
      "07d9a54d1353458ba6f9ff0063c8c545",
      "d5ec95957ae2471a9121961b3ebe75b7",
      "7cae80a80cbd4750b66870a986cabe81",
      "21bbb9e0cf454c61a2715ea9390d461b",
      "4cbd2bde42df4df68f5080b405c8a6f3",
      "34c78c3165f44d9fa6e6ce697c5b8318",
      "3f9b71759cc74cbb822fe4c1e790a584",
      "adce2d42d0cf476faae53526b4b6c33e",
      "b12fe4b260b141b7b2859c21201ad8f4",
      "185acc85b88448ed807304177a88e51b",
      "5ab630fa5caf419f97c35f05c1157cb0",
      "f450521fd29742598f8fa092c7a1f721",
      "f46a6f04b2b14a7e9ffd6b774cf9f4c0",
      "363a438176784660a55a623da344f601",
      "3b29b9a1d8c548a6ad4f49862408764c",
      "f026780de61b442a86b48aa946029d01",
      "0845ac70a49740b78422f1b9b3748a6a",
      "7aefe4fade9441fd8ceaa9e6a31a79cc",
      "72c84a7639f6437f9ac0ee7ded5b3dd5",
      "82775ee776f54e3a88ebc5c575618149",
      "3a5653d5563c4bc19c0eecfb50dcd492",
      "7876e4b5fc394f5fb60dddab896ed4e5",
      "a8dad801bc904e89804dd6a5833f8b48",
      "4e1659f292dc4289adf2753b66cea9d8",
      "455a09946bf74d85a853093b83074926",
      "df05f6531fa34ed0b36e4931a87b3da0",
      "90aed1459a774da0840814ad42f0fe42",
      "5d4ff8d0862145cba8795224dc631303",
      "17b71451850a421194bcafb3228d6041",
      "77947760c4f54c0795df9df5b59d1b0b",
      "e2a2882339034c7ebf64f1a726c42d3e",
      "71b84e7e7c6348ae8c5fd2afd8680cf4",
      "b0e7a33bf4e64b5488c9bb6070ea8822",
      "9021ef2370214b8ea526a2314570da91",
      "3f0d1c1d43bf4336b61c5ffbf7f7976f",
      "ad033f75ceee4b6e8003f9a6785b9adb",
      "489be99124ce46aab82c6dab40bb047b",
      "e92df2b11daf4251b882c94ff554a7f8",
      "652c7ef7d30c493fb71b6676a17ceb46",
      "24fd5a574f664fd386d6f5fe333757f1",
      "10c3fc0ba749475cab2cc459519ed733",
      "e3c4b6391ca6470fb6bf4572e43fab49",
      "aac3ad512cb04084a628aa48a6f0066f",
      "e2f21a87bd5a4f36a8d748856fb0ee81",
      "ba5576f525a54b05b3997681915a7421",
      "1a0f043b0d624462afd29904d19bbeee",
      "502b6bed5f8e4d9ca15899e72a380635"
     ]
    },
    "executionInfo": {
     "elapsed": 31122,
     "status": "ok",
     "timestamp": 1770212738450,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "GF-YEhYbtiq7",
    "outputId": "2bcfed41-bf15-49bd-cd13-5442acd01ae5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5630b31acd7f4723a109bf3dfe78d91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/795 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a411c674a6345af963ac668ca1faa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70437e707f7145ada0c0cc43c791b8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d693cce64c24c069028fd5f15987712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21bbb9e0cf454c61a2715ea9390d461b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b29b9a1d8c548a6ad4f49862408764c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df05f6531fa34ed0b36e4931a87b3da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489be99124ce46aab82c6dab40bb047b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name=\"BAAI/bge-reranker-v2-m3\"\n",
    "reranker=CrossEncoder(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1770213042394,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "iK6Zj1OPxQLP",
    "outputId": "017682b4-5e0c-4540-f392-eb44d2da6324"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1206998908.py:1: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class CrossEncoderRetriever(BaseRetriever, BaseModel):\n"
     ]
    }
   ],
   "source": [
    "class CrossEncoderRetriever(BaseRetriever, BaseModel):\n",
    "  vectorstore:Any = Field(..., description=\"The vectorstore for initial retrieval\")\n",
    "  reranker:Any = Field(..., description=\"model for rreranking\")\n",
    "  k:int = Field(default=5, description=\"Number of documents to retrieve initially\")\n",
    "  rerank_top_k: int = Field(default=3, description=\"Number of documents to return after reranking\")\n",
    "\n",
    "  #這是一個 告訴 Pydantic 放鬆型別檢查的開關,讓上面的vectorstore:Any不屬於python的內建型別\n",
    "  class Config:\n",
    "    arbitrary_types_allowed = True\n",
    "\n",
    "  def _get_relevant_documents(self, query:str) -> List[Document]:\n",
    "    initial_docs = self.vectorstore.similarity_search(query, k=self.k)\n",
    "    pair=[[query,doc.page_content] for doc in initial_docs]\n",
    "    scores=self.reranker.predict(pair)\n",
    "\n",
    "    scored_docs=sorted(zip(initial_docs,scores),key=lambda x:x[1],reverse=True)\n",
    "    return [doc for doc,_ in scored_docs[:self.rerank_top_k]]\n",
    "\n",
    "  async def aget_relevant_documents(self, query: str) -> List[Document]:\n",
    "        raise NotImplementedError(\"Async retrieval not implemented\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1770213118210,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "cUKO8TSW58YF"
   },
   "outputs": [],
   "source": [
    "# Create the cross-encoder retriever\n",
    "cross_encoder_retriever = CrossEncoderRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    reranker=reranker,\n",
    "    k=10,  # Retrieve 10 documents initially\n",
    "    rerank_top_k=5  # Return top 5 after reranking\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770213492084,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "1isZr04L8kB4"
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1770213724491,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "ilSTKZBA7K78"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "template=\"\"\"\n",
    "Answer the question base on content\n",
    "question:{question}\n",
    "context:{context}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": cross_encoder_retriever | format_docs,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 106,
     "status": "ok",
     "timestamp": 1770213980566,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "f768qdLx9i84",
    "outputId": "b2b2c628-34d7-4515-91f5-a7fcdeb99266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        6653  0.5  0.3 1859496 50656 ?       Sl   14:04   0:00 ollama serve\n",
      "root        6761  8.9  6.2 48197524 837024 ?     Sl   14:04   0:09 /usr/local/bin/ollama runner --ollama-engine --model /content/drive/MyDrive/RAG/ollama_models/blobs/sha256-af6e43ab13611311226e6f809f6a39b1a87b6df613bbf79468059f92ce819c4a --port 44439\n",
      "root        7213  0.0  0.0   7376  3456 ?        S    14:06   0:00 /bin/bash -c ps aux | grep ollama\n",
      "root        7215  0.0  0.0   6484  2596 ?        S    14:06   0:00 grep ollama\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 65816,
     "status": "ok",
     "timestamp": 1770213939743,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Ui4JjR5P8yTH",
    "outputId": "a43aa1f7-6a06-4a84-edc2-0b01671c7e2a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'CRAG proposed to improve the **ability of automatic self-correction** and the **efficient utilization of retrieved documents** in Retrieval-Augmented Generation (RAG) systems. Specifically, it addresses the issue of inaccurate retrieval results by introducing a lightweight retrieval evaluator that assesses the quality of retrieved documents. Based on this evaluation, a confidence degree is quantified to trigger three knowledge retrieval actions, thereby correcting flawed retrieval outcomes and ensuring only relevant and reliable information is used for generation. This enhances both the robustness and performance of RAG, especially in short- and long-form generation tasks.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOr6sHD0Z3dDvBHRvELsX/u",
   "gpuType": "T4",
   "mount_file_id": "1Bboso5YkqmvEtxLFxlIhQcZPT3ufisnB",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
