{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"Hierarchical Indices.ipynb\"\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "nbformat.write(nb, notebook_path)\n",
    "\n",
    "print(\"Fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 7317,
     "status": "ok",
     "timestamp": 1770295771383,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "KzDeQASzyq4B",
    "outputId": "bc4c4ced-05a8-4b65-fa82-9e8b539b4354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: pypdf, ollama, langchain-text-splitters, langchain-ollama, langchain-huggingface, langchain-community, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2 langchain-community-0.4.1 langchain-huggingface-1.2.0 langchain-ollama-1.0.1 langchain-text-splitters-1.1.0 ollama-0.6.1 pypdf-6.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters  pypdf  langchain-huggingface sentence-transformers faiss-cpu ollama langchain-ollama   --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 30480,
     "status": "ok",
     "timestamp": 1770295801865,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "YZbx5yRgyvrm",
    "outputId": "c0050fbd-0142-46be-bd9a-a076fbd9b5a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.3/566.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.2/553.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 第一個 Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# 注意：這裡刻意不寫 torch，直接用 Colab 內建的，就不會打架也不會慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "2467360f154c4729af53e8b9970becde",
      "4a6ea1da2bc742b8b63f93fc5bb25774",
      "73a587dc750c46fc977d8a378faab15a",
      "532e56dc851842ecb672c6f3659ea518",
      "1034817d5d5a46688467b89f4f8ff978",
      "97c5b8eca5774466ba574fdaf1d4e9fd",
      "4955f34e01ef4bd284e9d29213bd5e1c",
      "b5046d2c6f0d4bd59a40d76195100e8c",
      "51b4b21d5e604c5c8fbb7dac0e388286",
      "5f73aed177ab458693d1e53f0526703b",
      "6f3fcef0ad2c4efdbb6c55ba0fa38fdb",
      "05efd9474ef740b780985439738f2d2b",
      "51b3c924da08417d9d47f5670781d9d0",
      "fbd10cc2c67940959f491b14ca1e22e7",
      "7960829500a344568ffa04198e37c5d5",
      "72ca7899b1c740a395e8ee0fcfe92ea7",
      "47a085ccc9524f7eb043b9dacc62690e",
      "d955a2a49aeb4e39adcf93205c25eb63",
      "cfa5429dcb3344dea7772ce679d89319",
      "8f2b9c55b77143649e49fe81c9b199a1",
      "7c4d3500b4f44852a97d73895919c3f9",
      "06f90e702cd94150bd1e83ecae52f88e",
      "ef7aeb75c5984cbd8b4f25666711e221",
      "a45d41446018422e871093caebbf965e",
      "95706c4bad5d4c288dac35b79ee422c5",
      "4cf3c31d6caf448784d6c18f11d2ed3f",
      "0e568e6eb6e745fe9e71365741bb7a21",
      "ed353bf9d8fb4bc086afc31e3ff60aa2",
      "bbe94fb014494bdb95b6c29ddfb4e85f",
      "53be2b231598409799bf5a3cab48f752",
      "ef372d8fa185448d9d96ec2b27c0b174",
      "ffd5b00e565c40feb1dbfc1a2c92ceaf",
      "ab795db8fe8c47efa6dd5497ef57a10e",
      "71e639ff243d45988f672057a1b3c98d",
      "9be35e9a494e487cad5f1b1cc81182a4",
      "093531707ba4414f8f712765ff0b97f7",
      "e88e4d91bafb4fe18905878ae209b9f1",
      "587dd2701e4d44578cab7391a6517422",
      "1e8201c3072642b69b12267441eab143",
      "69a53640a4444526be6e64909717d198",
      "70498eca68d34530a1bf44dc95b54deb",
      "7f0a9089dcd341ba95f45cd090f24fce",
      "696dab4dc80a43609cfb1f2ea7817e8c",
      "0ee2cdc0e524458e97966147e5c04fd5",
      "4ad1f249c3214479854850392de9ca1e",
      "9cf25031cd9047108f30f2ae06207540",
      "19fced21464a434fb9b83cccc79e7d35",
      "9afcb43199b9414a9671efbef928da98",
      "b2e63b3e64654a77b35664068d9c08c5",
      "eddc56ac2ea643488ffab856762ca575",
      "f2a89b7b2def4b6bade5869d73100627",
      "23e5ab4c7ce94682a61c6ad02650e504",
      "b0b51357d0354faa8462fa314a09358a",
      "9bb569502ff347a7a866221840eb3e13",
      "816bd1efab47401288df40da2a993b64",
      "cdefb3d1c5ba40bd9492ce64a682472e",
      "de4a0f6a1af14e5faa4efc7719b78be1",
      "cf496c459cf7469ca6df19a510d30168",
      "33f00f37071741dba59da050a1cddcb0",
      "e9230376757c4370b701061cd4ce55a7",
      "1bdaa46d9ac74accaea5cbbee71c35dc",
      "06eb506df401447ebb0bf91af84e73dc",
      "3b5d915aa23142949190317a4f5bc31d",
      "15ec3be310a74716b1db992eed73ce88",
      "a541100058134c1584ba853df19cb2e0",
      "9a053e2fba0d4030b3b8d1f1edce8f2e",
      "519e221e4f934070b0b821f5845187fc",
      "e36d2134444f4ce38fc1a9c068387a54",
      "1da9e6ce27d24531aa899e4ccffae48c",
      "2b018ad620584e2ca086eba62d896bc2",
      "470a76a171994d6e98337fd0c15d40ea",
      "1ee82d56c2e74716a8a0857b04ad1777",
      "dd344fb7d1974733a0261047e7ca563f",
      "b2a7e26e468c4f059a699ffc05bbd3f9",
      "990c0233feab4a7ca57b5a08141fd60f",
      "e057bd1ecdb84891a6bdc74723a58dde",
      "223520b5b78e440b877afe3023c31adf",
      "715467d71733484d83f8671cbe219e09",
      "1d1f99f00a63402cb4e484f42ea64f90",
      "678d1429d0a54318bd4670cc23a12c34",
      "995d015a327d4edebd6f4dbdf3ce0aa8",
      "6a1262c659094ea986513fe823c3beae",
      "47fa0d5f9e13445bada8f42d248c9ac2",
      "6ea74fd6454d4d8ca6496a1495196935",
      "4db01e71c12d443ca676a8095cf5c4e6",
      "14a2c5659f684d019423a5a20bcf9ce6",
      "649badf60ece4d15914ae876083bd342",
      "4425755932974894b48a2652c270874d",
      "ac5de856ca124c6c8a809620f090e483",
      "548d782abb8b4711b9fe894171637fb0",
      "38882fb32b4a4b80b5d4fd5d5ec513cd",
      "81cc2f0526a940e8bb98803010d5c8c3",
      "1aaf9535d0a146d7a714929d421446e4",
      "8b5b02691f774fe09d59b8a020ffa7a7",
      "9d329bc4e6e3462e82e904dda830543a",
      "d122076ac84a421282a6f6a118b965aa",
      "8aabf55ab63d4c18ab2f3488816ae14e",
      "d6273ad9c4774ce986f4ffd361346242",
      "5a77917ae12940f2967700513a0d3b05",
      "9a67d188561d4f93a3e93c1988982f96",
      "24c1866ac55940dba5120fd0816692ca",
      "661e5282097b433eb6e00d57d55e4438",
      "cf60755bfa3f4878919ab9fc844355b4",
      "e7f2fa7fc8da422485a642335915dbcf",
      "4df2dbf26f314af9a9f9bbf6d0b9b77e",
      "cbf96d5254ed4fa0ac298e066efc4f6f",
      "bed87e264c62438baefdc3e12d73fb02",
      "134ef7b58fb949549a8f0478f42dd40a",
      "e40f318b12044fa29e7413901bcd4b26",
      "df2e6fb83b99469c8c1f6ebfd07f29cc"
     ]
    },
    "executionInfo": {
     "elapsed": 156001,
     "status": "ok",
     "timestamp": 1770295957873,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "kYm2TF8oy5mz",
    "outputId": "1756faa0-4dd4-449b-a948-c0d8008f2a8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2467360f154c4729af53e8b9970becde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05efd9474ef740b780985439738f2d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7aeb75c5984cbd8b4f25666711e221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e639ff243d45988f672057a1b3c98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad1f249c3214479854850392de9ca1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdefb3d1c5ba40bd9492ce64a682472e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519e221e4f934070b0b821f5845187fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715467d71733484d83f8671cbe219e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5de856ca124c6c8a809620f090e483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a67d188561d4f93a3e93c1988982f96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'max_new_tokens', 'repetition_penalty'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 確保 pipeline 正常運作的常見修復\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# ==========================================\n",
    "# 建立原生 Transformers Pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,      # 決定回答長度\n",
    "    temperature=0.2,         # RAG 建議設低一點，減少幻覺\n",
    "    repetition_penalty=1.1,  # 避免鬼打牆\n",
    "    return_full_text=False   # 重要：設為 False 只回傳生成的部分\n",
    ")\n",
    "\n",
    "# . 轉為 LangChain 的 LLM 物件\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# (關鍵) 轉為 ChatModel 物件\n",
    "# ChatHuggingFace 會自動處理 prompt template (User/System/Assistant 格式)\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26151,
     "status": "ok",
     "timestamp": 1770296001416,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "_bIamIx13Jma",
    "outputId": "aa8c4959-f4e9-4df1-b46a-fc2eec951427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "ff729eeb382f446b8fa90eaa0f42f876",
      "96a1b969f3644ff8a6904c63ea0bc446",
      "058ef90d9c3f428c918f4be76afe112e",
      "8b7c67159d404242bb082d045a139fee",
      "fe6b9e2fc4f1417094c262afd38df9a4",
      "70494de2fad04d759949dcb25b2c270b",
      "28332b1c5e734afdbd604d3ced67ef7f",
      "9fce44eb448c47d78afbb95dad1e1bc7",
      "0efba3a2cd0f4896b93d5a2e49fd063c",
      "8b0894a0f8cc42ae82b72f8280c98794",
      "337aba2aae444e3ba31c4812666077ce",
      "83a4091c992b40cd9d4177c0c7478857",
      "a33b258d2ee747f1892996f6fe05f0f6",
      "7c6bbcf73b444611a808f469bd2644c8",
      "1018aec0033c49b6aa2bc974e063813c",
      "5e03062cb9da4ca288a8a1f8f5cb23ae",
      "76455201ce054f5cac3a81abd28332c7",
      "488315e788014e8ca160bb68d4e9e6a4",
      "353a1ade8a0f4bb5bafdc910dd538658",
      "8d8f08b8260f428994e5a48b767e4caf",
      "33fb56b8f342448eaaa66c01f9843d31",
      "c4d4c994154b4ce0b370b904b10e40fc",
      "8a33be04130b4a1a8eaa27cd3a171bb5",
      "510c87852e414c88989c4b3088fb2c8a",
      "07924e7509814c61964b4d26efba9d02",
      "a1d5b94192bc4d23a167e3eaf23eca23",
      "23cdeb86294248ba84c52f0e1bb97d60",
      "c70fde65445b44eebc2dcf636be114d8",
      "e79c1fd477894bc49375507005a2c52f",
      "669f7077abca4b48884f92d4a023046a",
      "00e6bd2259fc4897b5986b756e98f592",
      "b9bb56cbe73947869ee12456b0fcf5aa",
      "7782679077f54b579b66b254905ded58",
      "60e5fe8a8447410eb3c9acd71c092e79",
      "14a82a810a304347930649e6f6d57342",
      "6cf36e35b52348e9a7bf75690b4c4666",
      "ca6cdad3b3e744ef90e93034790b20eb",
      "df02a1329f784a2fab6de486058dbffa",
      "59fa2950af86406892f50c5cc6b76d20",
      "d5ea17fce05247c5bdaefc7e52c109b3",
      "74dcd712a685402ca6825458eba5647d",
      "1ae8de08dcbc41ce9ff4fb5e551230e8",
      "6d054c8df0d047f68a8809202543846f",
      "4bed4568c3264096b4589788164e68f5",
      "973beb6b4db7485ca9579989ce82cbe6",
      "60a80da638034d09a85a7a83c52b53c2",
      "93890d32261a4515a6e007f5283db6c6",
      "d9ecdc91a6d846bab0e8a8c4b8c2fe42",
      "750a372d2a004c46b7c6aa49fcd40590",
      "831afcb2c70a42dba54ac36729c50fef",
      "f1ae8834e0444cce82d86e881f2b59ad",
      "0602c190f941435bb29e904c369623e2",
      "839978a50fcc47f2b87eaa04f85dbff8",
      "382073b6a6874f7abeeeaf57592e2fae",
      "2130692d11894364befd8d8a6ff4e049",
      "69f06fb5b01946e6abb445c2a7b15322",
      "2b1ea588df354713b4741efd5bb5116e",
      "a9da004610584a19bb2eeb75eeb192f9",
      "f2baad4010f44a7a81779e392c6da501",
      "afc91503444c4d96b0e9cfa28a17bb3d",
      "eb8d605dee784cd29d5621c9e5c8eb8f",
      "368b2d2246bf4428bebd0509dce5e905",
      "f6d20f6ee1c1416f94384dc25b1a5f38",
      "1fd1a759de07431f8f4698691e84a477",
      "06a94df1f7024d2fb8c7268c590cb91a",
      "5ea82664d7884e3bb9526300d52d275b",
      "e0a45304a5e740a28a936463f1a3e2ff",
      "2516d59079254853ad94391b63f9920f",
      "da3258c55a7d44a4bb1fe0f8895d48cb",
      "db0c7674cf88473c85cf42a3283fe763",
      "ca55025fd9f94825bdd7955cbfbe4862",
      "0f5ef08cf94644f78e07937145073eb4",
      "e940fd836dd04df488e80a22088b965d",
      "eb9bacdde7f84dac81accdafb404f063",
      "ff50413d39de4a91be83342039e6cc43",
      "e746f9631abb42f6af5432468070b62a",
      "fc430c4f06064cc2bfbef1f8bea1b58f",
      "d88b9c6e3e2748719b70e80c8ce2c39f",
      "b8c76c751a6446a2ad08b92152e74a0f",
      "8d077519278f4ec2ba2a4ac94ab37cc9",
      "92de2b3f675a48e4b3ace742afa8958b",
      "5780eaa9428146f6ac3e451eb4b08989",
      "a4be63dcdcfd49b4a28e24fedd69b8c1",
      "7d110e6380ca43f49412adf6d6a01434",
      "21dcdc2d10084c63a0ba736904a5daef",
      "5b1b42ef147b4248969cc6d5cf538662",
      "95ccc45bf89f427ea6b2216b8f44f92c",
      "d4aeb247e152416c9f999bcc05936069",
      "b45ef812da3641d9a0e1d2bd3fb08c56",
      "c2c28c44dd22445db1a22e2512a7b6b2",
      "760e12441eb3401f841fad2f218d129c",
      "4d3cb63cd8b446f69e526bf796187001",
      "698e051e72f5494690b2e605a9f9bb77",
      "9c38253c67734775a3ace156fd623b33",
      "94d7361b7c1f4390adc317d3164c73b4",
      "75afa03232354c55b16bd35b4ccd1233",
      "207205cc276846d4a6c46f0cd2e3fc7e",
      "0f9228076be346eb97d0f402df93bf0b",
      "d3a3cf208eb64daca7b0f14f5012b1e3",
      "f6132e3335f54f218ba07fcdbad13780",
      "777f02234e3141d98760ddfc182d9005",
      "e701cd952d494408a9f7750a34b9dea5",
      "01341e2824f84eba9dc7bc87c04ff6f9",
      "1134cd2263c6471490f925d368eed53f",
      "3fb571526c1049299968d926c0710302",
      "44e9f4e7547a46b0900f3287e6601e04",
      "934c4f540de44c2a938a47940c82c4e0",
      "303a691109d74b52af162ec7d4079031",
      "ee050504b1024ef7bedb4e47260f8cc9",
      "4e1e294a96604784a1e694300cfdb407",
      "00ee4e1f83fe4098baf370c51f5caa74",
      "9bfb672427f547f9b6f333f237ce8b5f",
      "e6dc8b2ab5784de5b54c89cf0318e4ec",
      "ddc1327888fe46a28e22915f8d000553",
      "4cd13cec2601433ba752ba392cc1083a",
      "35c94c8124db4389b5dbea88148bcf79",
      "f92a9a57326943c392c23dcbee3649f3",
      "2f4a57d5dd3b497f806ba4ff7d8a4162",
      "a3e2a070d8104a6ebef072097f843a70",
      "bdc5a33c095f4d629e10297a9ea4575d",
      "c0c4ff74ef2144eebe65180f9358e57a"
     ]
    },
    "executionInfo": {
     "elapsed": 35727,
     "status": "ok",
     "timestamp": 1770296181254,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "mFOxvW4ezEVT",
    "outputId": "0297fbdd-d81d-4c89-9e2f-35413f79f719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff729eeb382f446b8fa90eaa0f42f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a4091c992b40cd9d4177c0c7478857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a33be04130b4a1a8eaa27cd3a171bb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e5fe8a8447410eb3c9acd71c092e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973beb6b4db7485ca9579989ce82cbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f06fb5b01946e6abb445c2a7b15322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a45304a5e740a28a936463f1a3e2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88b9c6e3e2748719b70e80c8ce2c39f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45ef812da3641d9a0e1d2bd3fb08c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6132e3335f54f218ba07fcdbad13780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00ee4e1f83fe4098baf370c51f5caa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3752,
     "status": "ok",
     "timestamp": 1770296187026,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "b6a9WiKPynFF"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import asyncio\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1770296187044,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "_VF2Fhjyy1xj"
   },
   "outputs": [],
   "source": [
    "path=r\"/content/drive/MyDrive/RAG/RAG資料集/2401.15884v3.pdf\"\n",
    "chunk_size=1024\n",
    "chunk_overlap=128\n",
    "query=\"What the CRAG proposed to improve \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aozJZ8-M49ow"
   },
   "source": [
    "以頁數為單位做summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1770296357557,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "NtEW7If3zL6F"
   },
   "outputs": [],
   "source": [
    "async def encode_pdf_hierarchical(path, chunk_size, chunk_overlap, is_string=False):\n",
    "    #讀pdf\n",
    "    loader=PyPDFLoader(path)\n",
    "    documents= await asyncio.to_thread(loader.load) #[page1,page2...]\n",
    "    #summary chain\n",
    "    summary_template=\"\"\"\n",
    "    read the following context and write a brief summary:\n",
    "    {context}\n",
    "    summary:\n",
    "    \"\"\"\n",
    "    summary_prompt=ChatPromptTemplate.from_template(summary_template)\n",
    "    summary_chain= summary_prompt | chat_model | StrOutputParser()\n",
    "    #creat document-level summaries\n",
    "    async def summarized_doc(doc):\n",
    "        summary_text=summary_chain.invoke({\"context\":doc.page_content})\n",
    "\n",
    "        return Document(\n",
    "                page_content=summary_text,\n",
    "                metadata={\n",
    "                    \"source\": path,\n",
    "                    \"page\": doc.metadata[\"page\"],\n",
    "                    \"summary\": True\n",
    "                    }\n",
    "                )\n",
    "    summaries=[]\n",
    "    for doc in tqdm(documents):\n",
    "        result=await summarized_doc(doc)\n",
    "        if result:\n",
    "            summaries.append(result)\n",
    "\n",
    "    #creat detailed chunks\n",
    "    text_splitter=RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    detailed_chunks=await asyncio.to_thread(text_splitter.split_documents,documents)\n",
    "\n",
    "    for i,chunk in enumerate(detailed_chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\":i,\n",
    "            \"summary\":False,\n",
    "            \"page\":int(chunk.metadata.get(\"pate\",0)),\n",
    "        })\n",
    "\n",
    "    async def create_vectorstore(docs):\n",
    "        return await asyncio.to_thread(FAISS.from_documents,docs,embedding_model)\n",
    "\n",
    "    summary_vetorstore, detailed_vectorstore=await asyncio.gather(\n",
    "        create_vectorstore(summaries),\n",
    "        create_vectorstore(detailed_chunks)\n",
    "    )\n",
    "    return summary_vetorstore, detailed_vectorstore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcsehDF85F5V"
   },
   "source": [
    "建立FAISS Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231737,
     "status": "ok",
     "timestamp": 1770296591028,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Zgx3p6rSzdE4",
    "outputId": "35d2a799-68b6-48de-b9c7-8d86f5c0e798"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  6%|▋         | 1/16 [00:10<02:40, 10.72s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 12%|█▎        | 2/16 [00:21<02:27, 10.54s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 19%|█▉        | 3/16 [00:33<02:27, 11.36s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 25%|██▌       | 4/16 [00:47<02:29, 12.42s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 31%|███▏      | 5/16 [01:01<02:21, 12.91s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 38%|███▊      | 6/16 [01:14<02:08, 12.88s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 44%|████▍     | 7/16 [01:27<01:58, 13.18s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 50%|█████     | 8/16 [01:44<01:53, 14.20s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 56%|█████▋    | 9/16 [01:56<01:34, 13.49s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 62%|██████▎   | 10/16 [02:08<01:17, 12.97s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 69%|██████▉   | 11/16 [02:24<01:09, 13.89s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 75%|███████▌  | 12/16 [02:38<00:56, 14.16s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 81%|████████▏ | 13/16 [02:55<00:44, 14.98s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 88%|████████▊ | 14/16 [03:08<00:28, 14.19s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 94%|█████████▍| 15/16 [03:22<00:14, 14.41s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|██████████| 16/16 [03:34<00:00, 13.43s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "summary_path = \"/content/vector_stores/summary_store\"\n",
    "detailed_path = \"/content/vector_stores/detailed_store\"\n",
    "if os.path.exists(summary_path) and os.path.exists(detailed_path):\n",
    "\n",
    "    summary_store = FAISS.load_local(summary_path, embedding_model)\n",
    "    detailed_store = FAISS.load_local(detailed_path, embedding_model)\n",
    "else:\n",
    "    summary_store, detailed_store = await encode_pdf_hierarchical(path,chunk_size,chunk_overlap)\n",
    "    os.makedirs(summary_path, exist_ok=True)\n",
    "    os.makedirs(detailed_path, exist_ok=True)\n",
    "    summary_store.save_local(summary_path)\n",
    "    detailed_store.save_local(detailed_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mc4vxh8Q5KwI"
   },
   "source": [
    "先以頁為單位的summary做檢索,再以檢索到的頁數以chunk為單位做檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1770296894711,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Sn_sQE1z09ZM"
   },
   "outputs": [],
   "source": [
    "def retrieve_hierarchical(query, summary_vectorstore, detailed_vectorstore, k_summaries=2, k_chunks=5):\n",
    "  top_summaries=summary_vectorstore.similarity_search(query,k=k_summaries)\n",
    "  relevant_chunks=[]\n",
    "\n",
    "  for summary in top_summaries:\n",
    "    page_number=summary.metadata[\"page\"]\n",
    "    page_filter=lambda metadata: metadata[\"page\"]==page_number\n",
    "    page_chunks=detailed_vectorstore.similarity_search(query,k=k_chunks,filter=page_filter)\n",
    "    relevant_chunks.extend(page_chunks)\n",
    "  return relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1770296896139,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "FJ_m74d82xeN",
    "outputId": "c18bfe54-1d09-402b-920b-168e3f1744a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0\n",
      "Content: knowledge retrieval actions discriminately. With\n",
      "the further leverage of web search and optimized\n",
      "knowledge utilization, CRAG has significantly\n",
      "improved the ability of automatic self-correction\n",
      "and efficient utilization of retrieved documents.\n",
      "Experiments extensively demonstrate its adaptabil-\n",
      "ity to RAG-based approaches as well as general-\n",
      "izability across short- and long-form generation\n",
      "tasks. While we primarily proposed to improve the\n",
      "RAG framework from a corrective perspective and\n",
      "CRAG can be seamlessly coupled with various\n",
      "RAG-based approaches, fine-tuning an external\n",
      "retrieval evaluator is inevitable. How to eliminate\n",
      "this external evaluator and equip LLMs with better\n",
      "retrieval evaluation capabilities will be our future\n",
      "work.\n",
      "References\n",
      "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\n",
      "son, Dmitry Lepikhin, Alexandre Passos, Siamak\n",
      "Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\n",
      "Chen, Eric Chu, Jonathan H. Clark, Laurent El\n",
      "Shafey, Yanping Huang, Kathy Meier-Hellstern,...\n",
      "---\n",
      "Page: 0\n",
      "Content: Algorithm 1: CRAG Inference\n",
      "Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator)\n",
      "Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents)\n",
      "Output : y (Generated response)\n",
      "1 scorei = E evaluates the relevance of each pair (x, di), di ∈ D\n",
      "2 Confidence = Calculate and give a final judgment based on {score1, score2, ...scorek}\n",
      "// Confidence has 3 optional values: [CORRECT], [INCORRECT] or [AMBIGUOUS]\n",
      "3 if Confidence == [CORRECT] then\n",
      "4 Internal_Knowledge = Knowledge_Refine(x, D)\n",
      "5 k = Internal_Knowledge\n",
      "6 else if Confidence == [INCORRECT] then\n",
      "7 External_Knowledge = Web_Search(W Rewrites x for searching)\n",
      "8 k = External_Knowledge\n",
      "9 else if Confidence == [AMBIGUOUS] then\n",
      "10 Internal_Knowledge = Knowledge_Refine(x, D)\n",
      "11 External_Knowledge = Web_Search(W Rewrites x for searching)\n",
      "12 k = Internal_Knowledge + External_Knowledge\n",
      "13 end\n",
      "14 G predicts y given x and k\n",
      "4.3 Action Trigger\n",
      "To correct the irrelevant documents and refine the\n",
      "target documents as needed, actions should be exe-...\n",
      "---\n",
      "Page: 0\n",
      "Content: 7b. These results demonstrated the adaptability\n",
      "of CRAG which is plug-and-play and can be\n",
      "implemented into RAG-based approaches.\n",
      "Second, the proposed method demonstrated\n",
      "great generalizability across a variety of gen-\n",
      "eration tasks. In particular, these benchmarks\n",
      "reported in Table 1 respectively represent different\n",
      "practical scenarios including short-form entity\n",
      "generation (PopQA), long-form generation (Bi-\n",
      "ography), and closed-set tasks (PubHealth, Arc-\n",
      "Challenge). These results verified the consistent\n",
      "effectiveness ofCRAG . Its versatility across a spec-\n",
      "trum of tasks underscores its robust capabilities and\n",
      "generalizability across diverse scenarios.\n",
      "Third, the proposed method exhibited greater\n",
      "flexibility in replacing the underlying LLM gen-\n",
      "erator. It can be seen that CRAG still showed\n",
      "competitive performance when the underlying\n",
      "LLMs was changed from SelfRAG-LLaMA2-7b\n",
      "to LLaMA2-hf-7b, while the performance of Self-\n",
      "RAG dropped significantly, even underperforming...\n",
      "---\n",
      "Page: 0\n",
      "Content: same knowledge refinement method as Section 4.4\n",
      "to derive the relevant web knowledge, namely\n",
      "external knowledge.\n",
      "5 Experiments\n",
      "We conducted experiments to extensively demon-\n",
      "strate CRAG ’s adaptability to RAG-based ap-\n",
      "proaches and its generalizability across both short-\n",
      "and long-form generation tasks.\n",
      "5.1 Tasks, Datasets and Metrics\n",
      "CRAG was evaluated on four datasets, including\n",
      "PopQA (Mallen et al., 2023) ( short-form gener-\n",
      "ation), Biography (Min et al., 2023) ( long-form\n",
      "generation), PubHealth (Zhang et al., 2023a) (true-\n",
      "or-false question), and Arc-Challenge (Bhaktha-\n",
      "vatsalam et al., 2021) ( multiple-choice question).\n",
      "Following previous work, accuracy was adopted\n",
      "as the evaluation metric for PopQA, PubHealth,\n",
      "and Arc-Challenge. FactScore (Min et al., 2023)\n",
      "was adopted as the evaluation metric for Biography.\n",
      "Readers can refer to Appendix B.1 for more details.\n",
      "The same metrics are used because our proposed\n",
      "method is comparable to previous studies, since\n",
      "we used the same retrieval results as previous...\n",
      "---\n",
      "Page: 0\n",
      "Content: to complement and enrich the initially obtained\n",
      "documents. Furthermore, to eliminate redundant\n",
      "contexts contained in retrieved documents that are\n",
      "unhelpful for RAG, a decompose-then-recompose\n",
      "algorithm is meticulously crafted throughout the\n",
      "retrieval and utilization process. This algorithm\n",
      "ensures the refinement of retrieved information,\n",
      "optimizing the extraction of key insights and\n",
      "minimizing the inclusion of non-essential elements,\n",
      "thereby enhancing the utilization of retrieved data.\n",
      "CRAG is plug-and-play and experimentally\n",
      "implemented into RAG (Lewis et al., 2020) and\n",
      "Self-RAG (Asai et al., 2024) for demonstrating its\n",
      "adaptability to RAG-based approaches. Results on\n",
      "four datasets of PopQA (Mallen et al., 2023), Biog-\n",
      "raphy (Min et al., 2023), Pub Health (Zhang et al.,\n",
      "2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n",
      "2021) show that CRAG can significantly improve\n",
      "the performance of standard RAG and state-of-the-\n",
      "art Self-RAG, demonstrating its generalizability...\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "results=retrieve_hierarchical(query,summary_store,detailed_store)\n",
    "for chunk in results:\n",
    "    print(f\"Page: {chunk.metadata['page']}\")\n",
    "    print(f\"Content: {chunk.page_content}...\")\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt5xAcVqtcYWlrxC1+Eqhl",
   "gpuType": "T4",
   "mount_file_id": "1RyZqH0dWGe9J5ZxBNGyPIuf39prtVb6Z",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
