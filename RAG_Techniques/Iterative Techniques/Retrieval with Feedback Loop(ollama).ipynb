{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5027,
     "status": "ok",
     "timestamp": 1770556999069,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "zGVKSs4c-0Ym",
    "outputId": "9039cc44-fb69-4c1c-fa79-c8fbcac3c562"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.6.2)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
      "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.7)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: ollama, langchain-ollama\n",
      "Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters ollama langchain-ollama  pypdf  langchain-huggingface sentence-transformers faiss-cpu PyMuPDF   --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 35470,
     "status": "ok",
     "timestamp": 1770555241706,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "a9CJ2cNC_AKG",
    "outputId": "2e8b9115-4ed1-4890-d1fd-85d69a252df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸€å€‹ Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# æ³¨æ„ï¼šé€™è£¡åˆ»æ„ä¸å¯« torchï¼Œç›´æ¥ç”¨ Colab å…§å»ºçš„ï¼Œå°±ä¸æœƒæ‰“æ¶ä¹Ÿä¸æœƒæ…¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 18847,
     "status": "ok",
     "timestamp": 1770557027066,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Uni_gTwO_Ekj",
    "outputId": "b04c572f-6c52-4af5-a6e0-a63db25dc19d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Ollama æ¨¡å‹è·¯å¾‘å·²è¨­å®šç‚º: /content/drive/MyDrive/RAG/ollama_models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. æ›è¼‰ Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. è¨­å®šæ¨¡å‹å„²å­˜è·¯å¾‘ (å»ºè­°åœ¨ Drive å»ºç«‹ä¸€å€‹å°ˆé–€çš„è³‡æ–™å¤¾)\n",
    "# é€™è£¡è¨­å®šç‚º MyDrive ä¸‹çš„ RAG_project/ollama_models è³‡æ–™å¤¾\n",
    "my_model_folder = '/content/drive/MyDrive/RAG/ollama_models'\n",
    "\n",
    "# å¦‚æœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå»ºç«‹å®ƒ\n",
    "if not os.path.exists(my_model_folder):\n",
    "    os.makedirs(my_model_folder)\n",
    "\n",
    "# 3. ã€é—œéµã€‘è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œè®“ Ollama çŸ¥é“å»å“ªè£¡æ‰¾æ¨¡å‹\n",
    "os.environ['OLLAMA_MODELS'] = my_model_folder\n",
    "\n",
    "print(f\"Ollama æ¨¡å‹è·¯å¾‘å·²è¨­å®šç‚º: {os.environ['OLLAMA_MODELS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145770,
     "status": "ok",
     "timestamp": 1770557180983,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "0JOv41USbE1i",
    "outputId": "556440a3-6282-40f4-cf1a-92d2b8681898"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å®‰è£ç›¸ä¾å¥—ä»¶ zstd...\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  zstd\n",
      "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
      "Need to get 603 kB of archives.\n",
      "After this operation, 1,695 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n",
      "Fetched 603 kB in 1s (496 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package zstd.\n",
      "(Reading database ... 121689 files and directories currently installed.)\n",
      "Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n",
      "Unpacking zstd (1.4.8+dfsg-3build1) ...\n",
      "Setting up zstd (1.4.8+dfsg-3build1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "æ­£åœ¨å®‰è£ Ollama...\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading ollama-linux-amd64.tar.zst\n",
      "######################################################################## 100.0%\n",
      ">>> Creating ollama user...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
      "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "æ­£åœ¨å•Ÿå‹• Ollama æœå‹™...\n",
      "ç­‰å¾…æœå‹™å•Ÿå‹•ä¸­ (ç´„ 10 ç§’)...\n",
      "âœ… æˆåŠŸï¼šOllama æœå‹™å·²åœ¨èƒŒæ™¯åŸ·è¡Œï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "# --- 2. ä¿®æ­£ä¸¦å®‰è£ Ollama ---\n",
    "print(\"æ­£åœ¨å®‰è£ç›¸ä¾å¥—ä»¶ zstd...\")\n",
    "!sudo apt-get install -y zstd  # <--- æ–°å¢é€™ä¸€è¡Œè§£æ±ºä½ çš„éŒ¯èª¤\n",
    "\n",
    "print(\"æ­£åœ¨å®‰è£ Ollama...\")\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# --- 3. å•Ÿå‹• Ollama æœå‹™ ---\n",
    "print(\"æ­£åœ¨å•Ÿå‹• Ollama æœå‹™...\")\n",
    "# ä½¿ç”¨å®Œæ•´è·¯å¾‘ä»¥é˜²è¬ä¸€ (é€šå¸¸æ˜¯ /usr/local/bin/ollama)\n",
    "process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "# ç­‰å¾…æœå‹™å•Ÿå‹•\n",
    "print(\"ç­‰å¾…æœå‹™å•Ÿå‹•ä¸­ (ç´„ 10 ç§’)...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# --- 4. æ¸¬è©¦é€£ç·š ---\n",
    "try:\n",
    "    # æª¢æŸ¥æœå‹™æ˜¯å¦æ´»è‘—\n",
    "    check = subprocess.run([\"curl\", \"-s\", \"http://localhost:11434\"], capture_output=True, text=True)\n",
    "    if \"Ollama is running\" in check.stdout:\n",
    "        print(\"âœ… æˆåŠŸï¼šOllama æœå‹™å·²åœ¨èƒŒæ™¯åŸ·è¡Œï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ è­¦å‘Šï¼šæœå‹™ä¼¼ä¹æœªå›æ‡‰ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"æª¢æŸ¥é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1770557183698,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "geM8qYWybKqA"
   },
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "chat_model=ChatOllama(model=\"qwen3:4b-instruct-2507-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ecde0542d7ce48a3ad37324756659644",
      "93ee431256f04cc5a29ba9973b2c8357",
      "c138ebabd6ac40a1885786bffd437a16",
      "29b4bb6386814fe6b673b766ced062ff",
      "4a74bc8104b74805b8d73e78b2c3dfb2",
      "e4d4d23e9af64ebb84577cc5c0246bb7",
      "5cae6a2fc8da446a8fd63cf2c31ee5a7",
      "1dad88c57cbf4094ba1dca837a94cf94",
      "7ad83fbf68a74254b3e6eb2745a30e6f",
      "30f013925169496d8b0dd8e37938d217",
      "5ad8522e9c6e4d668a659d875786a1ba"
     ]
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5452,
     "status": "ok",
     "timestamp": 1770555861873,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "BDdBiFi-_ILT",
    "outputId": "4a3f2153-99a4-4d7f-f820-327ac5824467"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecde0542d7ce48a3ad37324756659644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 7402,
     "status": "ok",
     "timestamp": 1770555469137,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "EvIdwDnh_Lti"
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List,Tuple,Dict,Any\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import fitz\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import json\n",
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770555469147,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Qwb-FS78_NnE"
   },
   "outputs": [],
   "source": [
    "path=r\"/content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\"\n",
    "chunk_size=1024\n",
    "chunk_overlap=128\n",
    "query=\"What the CRAG proposed to improve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1770556125229,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "YEY16fzR_UD4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def replace_t_with_space(list_of_documents):\n",
    "\n",
    "\n",
    "    for doc in list_of_documents:\n",
    "        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n",
    "    return list_of_documents\n",
    "\n",
    "def encode_pdf(path, chunk_size, chunk_overlap):\n",
    "\n",
    "\n",
    "    # Load PDF documents\n",
    "    loader = PyPDFLoader(path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Split documents into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    cleaned_texts = replace_t_with_space(texts)\n",
    "    for chunk in cleaned_texts:\n",
    "        chunk.metadata['relevance_score'] = 1.0\n",
    "    # Create embeddings and vector store\n",
    "    vectorstore = FAISS.from_documents(cleaned_texts, embedding_model)\n",
    "\n",
    "    return vectorstore\n",
    "def encode_from_string(content, chunk_size, chunk_overlap):\n",
    "\n",
    "\n",
    "    if not isinstance(content, str) or not content.strip():\n",
    "        raise ValueError(\"Content must be a non-empty string.\")\n",
    "\n",
    "    try:\n",
    "        # Split the content into chunks\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            is_separator_regex=False,\n",
    "        )\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "\n",
    "        # Assign metadata to each chunk\n",
    "        for chunk in chunks:\n",
    "            chunk.metadata['relevance_score'] = 1.0\n",
    "\n",
    "        # Generate embeddings and create the vector store\n",
    "\n",
    "        vectorstore = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred during the encoding process: {str(e)}\")\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 18610,
     "status": "ok",
     "timestamp": 1770556148413,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "bmLQUrfl_W40"
   },
   "outputs": [],
   "source": [
    "vectorstore=encode_pdf(path,chunk_size,chunk_overlap)\n",
    "retriever=vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAtIGp5yD8l_"
   },
   "source": [
    "QA chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1770556150898,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "BSAJpEN8DWAD"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "QA_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1770557806994,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "UhfaFhoU_YAp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    return {\n",
    "        \"query\":query,\n",
    "        \"response\":response,\n",
    "        \"relevance\":relevance,\n",
    "        \"quality\":int(quality),\n",
    "        \"comments\":comments,\n",
    "    }\n",
    "\n",
    "def store_feedback(feedback):\n",
    "  folder_path = \"data\"\n",
    "  os.makedirs(folder_path, exist_ok=True)\n",
    "  file_path = os.path.join(folder_path, \"feedback_data.json\")\n",
    "  with open(\"data/feedback_data.json\", \"a\") as f: #\"a\" = append æ¨¡å¼ä¸æœƒè¦†è“‹åŸæœ¬è³‡æ–™\n",
    "      json.dump(feedback,f) #å°‡ Python è³‡æ–™å¯«å…¥æª”æ¡ˆï¼Œä¸¦ä»¥ JSON æ ¼å¼å„²å­˜\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "def load_feedback_data():\n",
    "    feedback_data=[]\n",
    "    try:\n",
    "        with open (\"data/feedback_data.json\", \"r\") as f:\n",
    "            for line in f: # ä¸€è¡Œä¸€è¡Œè®€\n",
    "                feedback_data.append(json.loads(line.strip())) #json.loads(...)å°‡ JSON å­—ä¸²è½‰å› Python dict\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64,
     "status": "ok",
     "timestamp": 1770558003894,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "K9Nc33xz_ZS6",
    "outputId": "3980ea61-d9d2-4526-da91-760678db25ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "<>:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "/tmp/ipython-input-3236151773.py:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n",
      "  if result is \"yes\":\n"
     ]
    }
   ],
   "source": [
    "class Response(BaseModel):\n",
    "    answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n",
    "\n",
    "def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n",
    "    template=\"\"\"\n",
    "        Determine if the following feedback response is relevant to the current query and document content.\n",
    "        You are also provided with the Feedback original query that was used to generate the feedback response.\n",
    "        Current query: {query}\n",
    "        Feedback query: {feedback_query}\n",
    "        Document content: {doc_content}\n",
    "        Feedback response: {feedback_response}\n",
    "\n",
    "        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n",
    "    \"\"\"\n",
    "    relevance_prompt=PromptTemplate.from_template(\n",
    "        template=template,\n",
    "        #input_variables=[\"query\",\"feedback_query\",\"doc_content\",\"feedback_response\"],\n",
    "    )\n",
    "\n",
    "    relevance_chain=relevance_prompt | chat_model.with_structured_output(Response)\n",
    "\n",
    "    for doc in docs:\n",
    "        relevance_feedback=[]\n",
    "        for feedback in feedback_data:\n",
    "            input_data={\n",
    "                \"query\":query,\n",
    "                \"feedback_query\":feedback[\"query\"],\n",
    "                \"doc_content\":doc.page_content[:1000],\n",
    "                \"feedback_response\":feedback[\"response\"]\n",
    "            }\n",
    "            result=relevance_chain.invoke(input_data).answer\n",
    "            if result is \"yes\":\n",
    "                relevance_feedback.append(feedback)\n",
    "        if relevance_feedback:\n",
    "            avg_relevance = sum(f['relevance'] for f in relevance_feedback) / len(relevance_feedback)\n",
    "            doc.metadata[\"relevance_score\"] =doc.metadata[\"relevance_score\"] *(avg_relevance / 3)\n",
    "\n",
    "    return sorted(docs ,key=lambda x : x.metadata[\"relevance_score\"],reverse=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1770557811286,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "1F3n_J4M_amx"
   },
   "outputs": [],
   "source": [
    "def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n",
    "    good_response=[f for f in feedback_data if f[\"relevance\"] >= 4 and f[\"quality\"] >= 4]\n",
    "\n",
    "    additional_texts=[]\n",
    "    for f in good_response:\n",
    "        conbined_text=f[\"query\"] +\"  \"+ f['response']\n",
    "        additional_texts.append(conbined_text)\n",
    "\n",
    "    additional_texts=\" \".join(additional_texts)\n",
    "\n",
    "    all_texts=texts + additional_texts\n",
    "    new_vectorstore=encode_from_string(all_texts)\n",
    "    return new_vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1770558326318,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "z8tPSu7Pbi2V",
    "outputId": "50a8c8a5-4fe0-492f-ad93-845081d429f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        9563  0.0  0.2 1785788 33496 ?       Sl   13:26   0:00 ollama serve\n",
      "root       14444  0.0  0.0   7376  3460 ?        S    13:45   0:00 /bin/bash -c ps aux | grep ollama\n",
      "root       14446  0.0  0.0   6484  2304 ?        R    13:45   0:00 grep ollama\n"
     ]
    }
   ],
   "source": [
    "!ps aux | grep ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8488,
     "status": "ok",
     "timestamp": 1770558320467,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "YivQItgGf0NH",
    "outputId": "c2f5af16-dd07-41cd-9936-111e40ea7383"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "response=QA_chain.invoke(query)\n",
    "relevance = 5\n",
    "quality = 5\n",
    "feedback = get_user_feedback(query, response, relevance, quality)\n",
    "store_feedback(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 661695,
     "status": "ok",
     "timestamp": 1770559005254,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "oR9RzBwLgAtu"
   },
   "outputs": [],
   "source": [
    "docs = retriever.invoke(query)\n",
    "adjusted_docs = adjust_relevance_scores(query, docs, load_feedback_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1770559174048,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "lcBKm8XmhWlP",
    "outputId": "18545c7b-ebd1-4101-bfe1-49e459b5972e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== åŸå§‹æª¢ç´¢çµæœ (Docs) (å…± 4 ç­†) ====================\n",
      "ğŸ“„ ç¬¬ 1 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. Experiments extensively demonstrate its adaptabil- ity t...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 2 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: Algorithm 1: CRAG Inference Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator) Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents) Output : y (Generated response) 1 scorei = E evaluates the relevance of each pair (x, di), di âˆˆ D 2 Confidence = Calculate and give ...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 3 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: 7b. These results demonstrated the adaptability of CRAG which is plug-and-play and can be implemented into RAG-based approaches. Second, the proposed method demonstrated great generalizability across a variety of gen- eration tasks. In particular, these benchmarks reported in Table 1 respectively re...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 4 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: same knowledge refinement method as Section 4.4 to derive the relevant web knowledge, namely external knowledge. 5 Experiments We conducted experiments to extensively demon- strate CRAG â€™s adaptability to RAG-based ap- proaches and its generalizability across both short- and long-form generation tas...\n",
      "--------------------------------------------------\n",
      "\n",
      "==================== èª¿æ•´å¾Œçµæœ (Adjusted Docs) (å…± 4 ç­†) ====================\n",
      "ğŸ“„ ç¬¬ 1 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. Experiments extensively demonstrate its adaptabil- ity t...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 2 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: Algorithm 1: CRAG Inference Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator) Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents) Output : y (Generated response) 1 scorei = E evaluates the relevance of each pair (x, di), di âˆˆ D 2 Confidence = Calculate and give ...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 3 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: 7b. These results demonstrated the adaptability of CRAG which is plug-and-play and can be implemented into RAG-based approaches. Second, the proposed method demonstrated great generalizability across a variety of gen- eration tasks. In particular, these benchmarks reported in Table 1 respectively re...\n",
      "--------------------------------------------------\n",
      "ğŸ“„ ç¬¬ 4 å\n",
      "   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n",
      "   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n",
      "   ğŸ”¹ å…§å®¹é è¦½: same knowledge refinement method as Section 4.4 to derive the relevant web knowledge, namely external knowledge. 5 Experiments We conducted experiments to extensively demon- strate CRAG â€™s adaptability to RAG-based ap- proaches and its generalizability across both short- and long-form generation tas...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def print_docs(title, documents):\n",
    "    print(f\"\\n{'='*20} {title} (å…± {len(documents)} ç­†) {'='*20}\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        # 1. å˜—è©¦å–å¾—åˆ†æ•¸ï¼Œå¦‚æœæ²’æœ‰å‰‡é¡¯ç¤º N/A\n",
    "        score = doc.metadata.get(\"relevance_score\", \"N/A\")\n",
    "\n",
    "        # 2. ç‚ºäº†ç‰ˆé¢æ•´æ½”ï¼Œå…§å®¹åªå°å‡ºå‰ 100 å€‹å­—\n",
    "        content_preview = doc.page_content[:300].replace('\\n', ' ')\n",
    "\n",
    "        print(f\"ğŸ“„ ç¬¬ {i+1} å\")\n",
    "        print(f\"   ğŸ”¹ åˆ†æ•¸ (Relevance Score): {score}\")\n",
    "        print(f\"   ğŸ”¹ ä¾†æº (Source): {doc.metadata.get('source', 'Unknown')}\")\n",
    "        print(f\"   ğŸ”¹ å…§å®¹é è¦½: {content_preview}...\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# --- åŸ·è¡Œåˆ—å° ---\n",
    "\n",
    "# 1. å°å‡ºåŸå§‹æª¢ç´¢çµæœ (é€šå¸¸é †åºæ˜¯ä¾ç…§å‘é‡ç›¸ä¼¼åº¦ï¼Œä¸”å¯èƒ½é‚„æ²’æœ‰ relevance_score)\n",
    "print_docs(\"åŸå§‹æª¢ç´¢çµæœ (Docs)\", docs)\n",
    "\n",
    "# 2. å°å‡ºèª¿æ•´å¾Œçš„çµæœ (æ‡‰è©²æœƒæœ‰åˆ†æ•¸ï¼Œä¸”é †åºå¯èƒ½æ”¹è®Š)\n",
    "print_docs(\"èª¿æ•´å¾Œçµæœ (Adjusted Docs)\", adjusted_docs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOc2Cu8kLN7zh6btC5KxLBj",
   "gpuType": "T4",
   "mount_file_id": "14cmYIr0dZhQlMspbxXLZIAKRVHXVR87I",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
