{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"14cmYIr0dZhQlMspbxXLZIAKRVHXVR87I","authorship_tag":"ABX9TyOc2Cu8kLN7zh6btC5KxLBj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"ecde0542d7ce48a3ad37324756659644":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_93ee431256f04cc5a29ba9973b2c8357","IPY_MODEL_c138ebabd6ac40a1885786bffd437a16","IPY_MODEL_29b4bb6386814fe6b673b766ced062ff"],"layout":"IPY_MODEL_4a74bc8104b74805b8d73e78b2c3dfb2"}},"93ee431256f04cc5a29ba9973b2c8357":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4d4d23e9af64ebb84577cc5c0246bb7","placeholder":"â€‹","style":"IPY_MODEL_5cae6a2fc8da446a8fd63cf2c31ee5a7","value":"Loadingâ€‡weights:â€‡100%"}},"c138ebabd6ac40a1885786bffd437a16":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dad88c57cbf4094ba1dca837a94cf94","max":310,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ad83fbf68a74254b3e6eb2745a30e6f","value":310}},"29b4bb6386814fe6b673b766ced062ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30f013925169496d8b0dd8e37938d217","placeholder":"â€‹","style":"IPY_MODEL_5ad8522e9c6e4d668a659d875786a1ba","value":"â€‡310/310â€‡[00:01&lt;00:00,â€‡322.54it/s,â€‡Materializingâ€‡param=norm.weight]"}},"4a74bc8104b74805b8d73e78b2c3dfb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4d4d23e9af64ebb84577cc5c0246bb7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cae6a2fc8da446a8fd63cf2c31ee5a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1dad88c57cbf4094ba1dca837a94cf94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad83fbf68a74254b3e6eb2745a30e6f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30f013925169496d8b0dd8e37938d217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ad8522e9c6e4d668a659d875786a1ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"zGVKSs4c-0Ym","executionInfo":{"status":"ok","timestamp":1770556999069,"user_tz":-480,"elapsed":5027,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"9039cc44-fb69-4c1c-fa79-c8fbcac3c562"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n","Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n","Collecting ollama\n","  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n","Collecting langchain-ollama\n","  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.6.2)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n","Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.12/dist-packages (1.26.7)\n","Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n","Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n","Installing collected packages: ollama, langchain-ollama\n","Successfully installed langchain-ollama-1.0.1 ollama-0.6.1\n"]}],"source":["!pip install langchain-community langchain-text-splitters ollama langchain-ollama  pypdf  langchain-huggingface sentence-transformers faiss-cpu PyMuPDF   --no-deps"]},{"cell_type":"code","source":["# ç¬¬ä¸€å€‹ Cell\n","!pip install -q bitsandbytes langchain-huggingface sentencepiece\n","!pip install -q -U transformers accelerate\n","# æ³¨æ„ï¼šé€™è£¡åˆ»æ„ä¸å¯« torchï¼Œç›´æ¥ç”¨ Colab å…§å»ºçš„ï¼Œå°±ä¸æœƒæ‰“æ¶ä¹Ÿä¸æœƒæ…¢"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"a9CJ2cNC_AKG","executionInfo":{"status":"ok","timestamp":1770555241706,"user_tz":-480,"elapsed":35470,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"2e8b9115-4ed1-4890-d1fd-85d69a252df2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","\n","# 1. æ›è¼‰ Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. è¨­å®šæ¨¡å‹å„²å­˜è·¯å¾‘ (å»ºè­°åœ¨ Drive å»ºç«‹ä¸€å€‹å°ˆé–€çš„è³‡æ–™å¤¾)\n","# é€™è£¡è¨­å®šç‚º MyDrive ä¸‹çš„ RAG_project/ollama_models è³‡æ–™å¤¾\n","my_model_folder = '/content/drive/MyDrive/RAG/ollama_models'\n","\n","# å¦‚æœè³‡æ–™å¤¾ä¸å­˜åœ¨ï¼Œå»ºç«‹å®ƒ\n","if not os.path.exists(my_model_folder):\n","    os.makedirs(my_model_folder)\n","\n","# 3. ã€é—œéµã€‘è¨­å®šç’°å¢ƒè®Šæ•¸ï¼Œè®“ Ollama çŸ¥é“å»å“ªè£¡æ‰¾æ¨¡å‹\n","os.environ['OLLAMA_MODELS'] = my_model_folder\n","\n","print(f\"Ollama æ¨¡å‹è·¯å¾‘å·²è¨­å®šç‚º: {os.environ['OLLAMA_MODELS']}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"Uni_gTwO_Ekj","executionInfo":{"status":"ok","timestamp":1770557027066,"user_tz":-480,"elapsed":18847,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"b04c572f-6c52-4af5-a6e0-a63db25dc19d"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Ollama æ¨¡å‹è·¯å¾‘å·²è¨­å®šç‚º: /content/drive/MyDrive/RAG/ollama_models\n"]}]},{"cell_type":"code","source":["import os\n","import subprocess\n","import time\n","from google.colab import drive\n","\n","\n","# --- 2. ä¿®æ­£ä¸¦å®‰è£ Ollama ---\n","print(\"æ­£åœ¨å®‰è£ç›¸ä¾å¥—ä»¶ zstd...\")\n","!sudo apt-get install -y zstd  # <--- æ–°å¢é€™ä¸€è¡Œè§£æ±ºä½ çš„éŒ¯èª¤\n","\n","print(\"æ­£åœ¨å®‰è£ Ollama...\")\n","!curl -fsSL https://ollama.com/install.sh | sh\n","\n","# --- 3. å•Ÿå‹• Ollama æœå‹™ ---\n","print(\"æ­£åœ¨å•Ÿå‹• Ollama æœå‹™...\")\n","# ä½¿ç”¨å®Œæ•´è·¯å¾‘ä»¥é˜²è¬ä¸€ (é€šå¸¸æ˜¯ /usr/local/bin/ollama)\n","process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","\n","# ç­‰å¾…æœå‹™å•Ÿå‹•\n","print(\"ç­‰å¾…æœå‹™å•Ÿå‹•ä¸­ (ç´„ 10 ç§’)...\")\n","time.sleep(10)\n","\n","# --- 4. æ¸¬è©¦é€£ç·š ---\n","try:\n","    # æª¢æŸ¥æœå‹™æ˜¯å¦æ´»è‘—\n","    check = subprocess.run([\"curl\", \"-s\", \"http://localhost:11434\"], capture_output=True, text=True)\n","    if \"Ollama is running\" in check.stdout:\n","        print(\"âœ… æˆåŠŸï¼šOllama æœå‹™å·²åœ¨èƒŒæ™¯åŸ·è¡Œï¼\")\n","    else:\n","        print(\"âš ï¸ è­¦å‘Šï¼šæœå‹™ä¼¼ä¹æœªå›æ‡‰ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚\")\n","except Exception as e:\n","    print(f\"æª¢æŸ¥é€£ç·šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0JOv41USbE1i","executionInfo":{"status":"ok","timestamp":1770557180983,"user_tz":-480,"elapsed":145770,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"556440a3-6282-40f4-cf1a-92d2b8681898"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["æ­£åœ¨å®‰è£ç›¸ä¾å¥—ä»¶ zstd...\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following NEW packages will be installed:\n","  zstd\n","0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n","Need to get 603 kB of archives.\n","After this operation, 1,695 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 zstd amd64 1.4.8+dfsg-3build1 [603 kB]\n","Fetched 603 kB in 1s (496 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package zstd.\n","(Reading database ... 121689 files and directories currently installed.)\n","Preparing to unpack .../zstd_1.4.8+dfsg-3build1_amd64.deb ...\n","Unpacking zstd (1.4.8+dfsg-3build1) ...\n","Setting up zstd (1.4.8+dfsg-3build1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","æ­£åœ¨å®‰è£ Ollama...\n",">>> Installing ollama to /usr/local\n",">>> Downloading ollama-linux-amd64.tar.zst\n","######################################################################## 100.0%\n",">>> Creating ollama user...\n",">>> Adding ollama user to video group...\n",">>> Adding current user to ollama group...\n",">>> Creating ollama systemd service...\n","\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n","\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",">>> The Ollama API is now available at 127.0.0.1:11434.\n",">>> Install complete. Run \"ollama\" from the command line.\n","æ­£åœ¨å•Ÿå‹• Ollama æœå‹™...\n","ç­‰å¾…æœå‹™å•Ÿå‹•ä¸­ (ç´„ 10 ç§’)...\n","âœ… æˆåŠŸï¼šOllama æœå‹™å·²åœ¨èƒŒæ™¯åŸ·è¡Œï¼\n"]}]},{"cell_type":"code","source":["from langchain_ollama import ChatOllama\n","chat_model=ChatOllama(model=\"qwen3:4b-instruct-2507-q8_0\",temperature=0)"],"metadata":{"id":"geM8qYWybKqA","executionInfo":{"status":"ok","timestamp":1770557183698,"user_tz":-480,"elapsed":254,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n","embedding_model = HuggingFaceEmbeddings(\n","    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n","    model_kwargs={'device': 'cuda'},\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["ecde0542d7ce48a3ad37324756659644","93ee431256f04cc5a29ba9973b2c8357","c138ebabd6ac40a1885786bffd437a16","29b4bb6386814fe6b673b766ced062ff","4a74bc8104b74805b8d73e78b2c3dfb2","e4d4d23e9af64ebb84577cc5c0246bb7","5cae6a2fc8da446a8fd63cf2c31ee5a7","1dad88c57cbf4094ba1dca837a94cf94","7ad83fbf68a74254b3e6eb2745a30e6f","30f013925169496d8b0dd8e37938d217","5ad8522e9c6e4d668a659d875786a1ba"]},"collapsed":true,"id":"BDdBiFi-_ILT","executionInfo":{"status":"ok","timestamp":1770555861873,"user_tz":-480,"elapsed":5452,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"4a3f2153-99a4-4d7f-f820-327ac5824467"},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecde0542d7ce48a3ad37324756659644"}},"metadata":{}}]},{"cell_type":"code","source":["from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from typing import List,Tuple,Dict,Any\n","from langchain_community.document_loaders import PyPDFLoader\n","import fitz\n","from langchain_community.vectorstores import FAISS\n","import json\n","from pydantic import BaseModel,Field\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough"],"metadata":{"id":"EvIdwDnh_Lti","executionInfo":{"status":"ok","timestamp":1770555469137,"user_tz":-480,"elapsed":7402,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["path=r\"/content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\"\n","chunk_size=1024\n","chunk_overlap=128\n","query=\"What the CRAG proposed to improve\""],"metadata":{"id":"Qwb-FS78_NnE","executionInfo":{"status":"ok","timestamp":1770555469147,"user_tz":-480,"elapsed":2,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","def replace_t_with_space(list_of_documents):\n","\n","\n","    for doc in list_of_documents:\n","        doc.page_content = doc.page_content.replace('\\t', ' ')  # Replace tabs with spaces\n","    return list_of_documents\n","\n","def encode_pdf(path, chunk_size, chunk_overlap):\n","\n","\n","    # Load PDF documents\n","    loader = PyPDFLoader(path)\n","    documents = loader.load()\n","\n","    # Split documents into chunks\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n","    )\n","    texts = text_splitter.split_documents(documents)\n","    cleaned_texts = replace_t_with_space(texts)\n","    for chunk in cleaned_texts:\n","        chunk.metadata['relevance_score'] = 1.0\n","    # Create embeddings and vector store\n","    vectorstore = FAISS.from_documents(cleaned_texts, embedding_model)\n","\n","    return vectorstore\n","def encode_from_string(content, chunk_size, chunk_overlap):\n","\n","\n","    if not isinstance(content, str) or not content.strip():\n","        raise ValueError(\"Content must be a non-empty string.\")\n","\n","    try:\n","        # Split the content into chunks\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=chunk_size,\n","            chunk_overlap=chunk_overlap,\n","            length_function=len,\n","            is_separator_regex=False,\n","        )\n","        chunks = text_splitter.create_documents([content])\n","\n","        # Assign metadata to each chunk\n","        for chunk in chunks:\n","            chunk.metadata['relevance_score'] = 1.0\n","\n","        # Generate embeddings and create the vector store\n","\n","        vectorstore = FAISS.from_documents(chunks, embedding_model)\n","\n","    except Exception as e:\n","        raise RuntimeError(f\"An error occurred during the encoding process: {str(e)}\")\n","\n","    return vectorstore\n"],"metadata":{"id":"YEY16fzR_UD4","executionInfo":{"status":"ok","timestamp":1770556125229,"user_tz":-480,"elapsed":3,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["vectorstore=encode_pdf(path,chunk_size,chunk_overlap)\n","retriever=vectorstore.as_retriever()"],"metadata":{"id":"bmLQUrfl_W40","executionInfo":{"status":"ok","timestamp":1770556148413,"user_tz":-480,"elapsed":18610,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":["QA chain"],"metadata":{"id":"tAtIGp5yD8l_"}},{"cell_type":"code","source":["template = \"\"\"Answer the question based only on the following context:\n","{context}\n","\n","Question: {question}\n","\"\"\"\n","prompt = ChatPromptTemplate.from_template(template)\n","\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","QA_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | chat_model\n","    | StrOutputParser()\n",")"],"metadata":{"id":"BSAJpEN8DWAD","executionInfo":{"status":"ok","timestamp":1770556150898,"user_tz":-480,"elapsed":2,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import os\n","def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n","    return {\n","        \"query\":query,\n","        \"response\":response,\n","        \"relevance\":relevance,\n","        \"quality\":int(quality),\n","        \"comments\":comments,\n","    }\n","\n","def store_feedback(feedback):\n","  folder_path = \"data\"\n","  os.makedirs(folder_path, exist_ok=True)\n","  file_path = os.path.join(folder_path, \"feedback_data.json\")\n","  with open(\"data/feedback_data.json\", \"a\") as f: #\"a\" = append æ¨¡å¼ä¸æœƒè¦†è“‹åŸæœ¬è³‡æ–™\n","      json.dump(feedback,f) #å°‡ Python è³‡æ–™å¯«å…¥æª”æ¡ˆï¼Œä¸¦ä»¥ JSON æ ¼å¼å„²å­˜\n","      f.write(\"\\n\")\n","\n","def load_feedback_data():\n","    feedback_data=[]\n","    try:\n","        with open (\"data/feedback_data.json\", \"r\") as f:\n","            for line in f: # ä¸€è¡Œä¸€è¡Œè®€\n","                feedback_data.append(json.loads(line.strip())) #json.loads(...)å°‡ JSON å­—ä¸²è½‰å› Python dict\n","    except FileNotFoundError:\n","        print(\"No feedback data file found. Starting with empty feedback.\")\n","    return feedback_data"],"metadata":{"id":"UhfaFhoU_YAp","executionInfo":{"status":"ok","timestamp":1770557806994,"user_tz":-480,"elapsed":20,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["class Response(BaseModel):\n","    answer: str = Field(..., title=\"The answer to the question. The options can be only 'Yes' or 'No'\")\n","\n","def adjust_relevance_scores(query: str, docs: List[Any], feedback_data: List[Dict[str, Any]]) -> List[Any]:\n","    template=\"\"\"\n","        Determine if the following feedback response is relevant to the current query and document content.\n","        You are also provided with the Feedback original query that was used to generate the feedback response.\n","        Current query: {query}\n","        Feedback query: {feedback_query}\n","        Document content: {doc_content}\n","        Feedback response: {feedback_response}\n","\n","        Is this feedback relevant? Respond with only 'Yes' or 'No'.\n","    \"\"\"\n","    relevance_prompt=PromptTemplate.from_template(\n","        template=template,\n","        #input_variables=[\"query\",\"feedback_query\",\"doc_content\",\"feedback_response\"],\n","    )\n","\n","    relevance_chain=relevance_prompt | chat_model.with_structured_output(Response)\n","\n","    for doc in docs:\n","        relevance_feedback=[]\n","        for feedback in feedback_data:\n","            input_data={\n","                \"query\":query,\n","                \"feedback_query\":feedback[\"query\"],\n","                \"doc_content\":doc.page_content[:1000],\n","                \"feedback_response\":feedback[\"response\"]\n","            }\n","            result=relevance_chain.invoke(input_data).answer\n","            if result is \"yes\":\n","                relevance_feedback.append(feedback)\n","        if relevance_feedback:\n","            avg_relevance = sum(f['relevance'] for f in relevance_feedback) / len(relevance_feedback)\n","            doc.metadata[\"relevance_score\"] =doc.metadata[\"relevance_score\"] *(avg_relevance / 3)\n","\n","    return sorted(docs ,key=lambda x : x.metadata[\"relevance_score\"],reverse=True)\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K9Nc33xz_ZS6","executionInfo":{"status":"ok","timestamp":1770558003894,"user_tz":-480,"elapsed":64,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"3980ea61-d9d2-4526-da91-760678db25ad"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n","<>:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n","/tmp/ipython-input-3236151773.py:32: SyntaxWarning: \"is\" with 'str' literal. Did you mean \"==\"?\n","  if result is \"yes\":\n"]}]},{"cell_type":"code","source":["def fine_tune_index(feedback_data: List[Dict[str, Any]], texts: List[str]) -> Any:\n","    good_response=[f for f in feedback_data if f[\"relevance\"] >= 4 and f[\"quality\"] >= 4]\n","\n","    additional_texts=[]\n","    for f in good_response:\n","        conbined_text=f[\"query\"] +\"  \"+ f['response']\n","        additional_texts.append(conbined_text)\n","\n","    additional_texts=\" \".join(additional_texts)\n","\n","    all_texts=texts + additional_texts\n","    new_vectorstore=encode_from_string(all_texts)\n","    return new_vectorstore"],"metadata":{"id":"1F3n_J4M_amx","executionInfo":{"status":"ok","timestamp":1770557811286,"user_tz":-480,"elapsed":20,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["!ps aux | grep ollama"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z8tPSu7Pbi2V","executionInfo":{"status":"ok","timestamp":1770558326318,"user_tz":-480,"elapsed":114,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"50a8c8a5-4fe0-492f-ad93-845081d429f4"},"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["root        9563  0.0  0.2 1785788 33496 ?       Sl   13:26   0:00 ollama serve\n","root       14444  0.0  0.0   7376  3460 ?        S    13:45   0:00 /bin/bash -c ps aux | grep ollama\n","root       14446  0.0  0.0   6484  2304 ?        R    13:45   0:00 grep ollama\n"]}]},{"cell_type":"code","source":["response=QA_chain.invoke(query)\n","relevance = 5\n","quality = 5\n","feedback = get_user_feedback(query, response, relevance, quality)\n","store_feedback(feedback)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YivQItgGf0NH","executionInfo":{"status":"ok","timestamp":1770558320467,"user_tz":-480,"elapsed":8488,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"c2f5af16-dd07-41cd-9936-111e40ea7383"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stderr","text":["Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"]}]},{"cell_type":"code","source":["docs = retriever.invoke(query)\n","adjusted_docs = adjust_relevance_scores(query, docs, load_feedback_data())"],"metadata":{"id":"oR9RzBwLgAtu","executionInfo":{"status":"ok","timestamp":1770559005254,"user_tz":-480,"elapsed":661695,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["def print_docs(title, documents):\n","    print(f\"\\n{'='*20} {title} (å…± {len(documents)} ç­†) {'='*20}\")\n","\n","    for i, doc in enumerate(documents):\n","        # 1. å˜—è©¦å–å¾—åˆ†æ•¸ï¼Œå¦‚æœæ²’æœ‰å‰‡é¡¯ç¤º N/A\n","        score = doc.metadata.get(\"relevance_score\", \"N/A\")\n","\n","        # 2. ç‚ºäº†ç‰ˆé¢æ•´æ½”ï¼Œå…§å®¹åªå°å‡ºå‰ 100 å€‹å­—\n","        content_preview = doc.page_content[:300].replace('\\n', ' ')\n","\n","        print(f\"ğŸ“„ ç¬¬ {i+1} å\")\n","        print(f\"   ğŸ”¹ åˆ†æ•¸ (Relevance Score): {score}\")\n","        print(f\"   ğŸ”¹ ä¾†æº (Source): {doc.metadata.get('source', 'Unknown')}\")\n","        print(f\"   ğŸ”¹ å…§å®¹é è¦½: {content_preview}...\")\n","        print(\"-\" * 50)\n","\n","# --- åŸ·è¡Œåˆ—å° ---\n","\n","# 1. å°å‡ºåŸå§‹æª¢ç´¢çµæœ (é€šå¸¸é †åºæ˜¯ä¾ç…§å‘é‡ç›¸ä¼¼åº¦ï¼Œä¸”å¯èƒ½é‚„æ²’æœ‰ relevance_score)\n","print_docs(\"åŸå§‹æª¢ç´¢çµæœ (Docs)\", docs)\n","\n","# 2. å°å‡ºèª¿æ•´å¾Œçš„çµæœ (æ‡‰è©²æœƒæœ‰åˆ†æ•¸ï¼Œä¸”é †åºå¯èƒ½æ”¹è®Š)\n","print_docs(\"èª¿æ•´å¾Œçµæœ (Adjusted Docs)\", adjusted_docs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lcBKm8XmhWlP","executionInfo":{"status":"ok","timestamp":1770559174048,"user_tz":-480,"elapsed":18,"user":{"displayName":"314657023æŸç‘","userId":"02820414516176440519"}},"outputId":"18545c7b-ebd1-4101-bfe1-49e459b5972e"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","==================== åŸå§‹æª¢ç´¢çµæœ (Docs) (å…± 4 ç­†) ====================\n","ğŸ“„ ç¬¬ 1 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. Experiments extensively demonstrate its adaptabil- ity t...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 2 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: Algorithm 1: CRAG Inference Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator) Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents) Output : y (Generated response) 1 scorei = E evaluates the relevance of each pair (x, di), di âˆˆ D 2 Confidence = Calculate and give ...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 3 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: 7b. These results demonstrated the adaptability of CRAG which is plug-and-play and can be implemented into RAG-based approaches. Second, the proposed method demonstrated great generalizability across a variety of gen- eration tasks. In particular, these benchmarks reported in Table 1 respectively re...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 4 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: same knowledge refinement method as Section 4.4 to derive the relevant web knowledge, namely external knowledge. 5 Experiments We conducted experiments to extensively demon- strate CRAG â€™s adaptability to RAG-based ap- proaches and its generalizability across both short- and long-form generation tas...\n","--------------------------------------------------\n","\n","==================== èª¿æ•´å¾Œçµæœ (Adjusted Docs) (å…± 4 ç­†) ====================\n","ğŸ“„ ç¬¬ 1 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. Experiments extensively demonstrate its adaptabil- ity t...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 2 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: Algorithm 1: CRAG Inference Require :E (Retrieval Evaluator), W (Query Rewriter), G (Generator) Input : x (Input question), D = {d1, d2, ..., dk} (Retrieved documents) Output : y (Generated response) 1 scorei = E evaluates the relevance of each pair (x, di), di âˆˆ D 2 Confidence = Calculate and give ...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 3 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: 7b. These results demonstrated the adaptability of CRAG which is plug-and-play and can be implemented into RAG-based approaches. Second, the proposed method demonstrated great generalizability across a variety of gen- eration tasks. In particular, these benchmarks reported in Table 1 respectively re...\n","--------------------------------------------------\n","ğŸ“„ ç¬¬ 4 å\n","   ğŸ”¹ åˆ†æ•¸ (Relevance Score): 1.0\n","   ğŸ”¹ ä¾†æº (Source): /content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\n","   ğŸ”¹ å…§å®¹é è¦½: same knowledge refinement method as Section 4.4 to derive the relevant web knowledge, namely external knowledge. 5 Experiments We conducted experiments to extensively demon- strate CRAG â€™s adaptability to RAG-based ap- proaches and its generalizability across both short- and long-form generation tas...\n","--------------------------------------------------\n"]}]}]}