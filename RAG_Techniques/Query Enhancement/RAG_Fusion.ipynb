{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd11ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##indexing##\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "loader=WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300,chunk_overlap=50)\n",
    "splits=text_splitter.split_documents(docs) #->list\n",
    "\n",
    "#embed\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "vectorstore=Chroma.from_documents(documents=splits,embedding=embedding_model)\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "#ollama llm\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f173ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "#multi query prompts\n",
    "template=\"\"\"\n",
    "You are an AI language model assistant. Your task is to generate three\n",
    "different versions of the given user question to retrieve relevant documents from a vector\n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search.\n",
    "Provide these alternative questions separated by newlines. Original question: {question}\n",
    "\"\"\"\n",
    "prompt_rag_fusion=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_querier=(\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8c603",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import loads,dumps\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(results: list    [list],k=60):\n",
    "    #紀錄每個document的分數\n",
    "    fuse_score={}\n",
    "    for docs in results:\n",
    "        for rank,doc in enumerate(docs):\n",
    "\n",
    "            doc_str=dumps(doc) #:->str\n",
    "\n",
    "            #初始分數都為0\n",
    "            if doc_str not in fuse_score:\n",
    "                fuse_score[doc_str]=0\n",
    "\n",
    "            pre_score=fuse_score[doc_str]\n",
    "            #RRF formula: 1 / (rank + k)\n",
    "            fuse_score[doc_str]=pre_score+1/(rank+k)\n",
    "\n",
    "    #Sort the docs based on fused scores in descending order to get the final reranked results\n",
    "    reranked_results=[\n",
    "        (loads(doc),score)\n",
    "        for doc,score in sorted(fuse_score.items() ,key=lambda x : x[1] , reverse=True )\n",
    "        ]\n",
    "\n",
    "    return reranked_results\n",
    "\n",
    "question=\"What is task decomposition for LLM agents?\"\n",
    "\n",
    "retrieval_chain_rag_fusion= generate_querier | retriever.map() | reciprocal_rank_fusion\n",
    "\n",
    "#docs=retrieval_chain_rag_fusion.invoke({\"question\":question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c076c88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a technique used to break down complex tasks into smaller, simpler steps that an agent can plan and execute. This allows the agent to utilize more test-time computation and decompose hard tasks into manageable subtasks.\\n\\nThere are three ways to perform task decomposition:\\n\\n1. Simple prompting: The LLM is given simple prompts like \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ.\"\\n2. Task-specific instructions: The agent is given task-specific instructions, such as \"Write a story outline\" for writing a novel.\\n3. Human inputs: The agent receives human inputs to perform task decomposition.\\n\\nTask decomposition can be done using techniques like Chain of Thought (CoT) or Tree of Thoughts (Yao et al. 2023), which transform big tasks into multiple manageable tasks and shed light on the model\\'s thinking process.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "template=\"\"\"\n",
    "Answer the following question based in the context:\n",
    "{context}\n",
    "\n",
    "Question:{question}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain=(\n",
    "    {\n",
    "        \"context\":retrieval_chain_rag_fusion,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
