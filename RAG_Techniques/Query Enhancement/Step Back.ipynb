{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d324e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "##indexing##\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "loader=WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300,chunk_overlap=50)\n",
    "splits=text_splitter.split_documents(docs) #->list\n",
    "\n",
    "#embed\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "vectorstore=Chroma.from_documents(documents=splits,embedding=embedding_model)\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "#ollama llm\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123a9843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate,ChatPromptTemplate\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jerry was born in what country?\",\n",
    "        \"output\": \"what is Jerry personal history?\",\n",
    "    },\n",
    "]\n",
    "\n",
    "examples_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\",\"{input}\"),\n",
    "        (\"ai\",\"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt=FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=examples_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "\n",
    "prompt_step_back=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge.\n",
    "            Your task is to step back and paraphrase a question to a more generic step-back question,\n",
    "            which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "\n",
    "        few_shot_prompt,\n",
    "\n",
    "        (\n",
    "            \"user\",\"{question}\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f3506",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "generate_queries_step_back=prompt_step_back | llm | StrOutputParser()\n",
    "\n",
    "question=\"What is task decomposition for LLM agents\"\n",
    "\n",
    "normal_chain=(\n",
    "    itemgetter(\"question\")\n",
    "    | retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c2254f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a crucial component of planning in LLM-powered autonomous agent systems, where the large language model (LLM) breaks down complex tasks into smaller, manageable subgoals. This process enables efficient handling of complex tasks and allows the agent to plan ahead.\\n\\nThere are three ways task decomposition can be done:\\n\\n1. **Simple prompting**: The LLM is instructed with simple prompts like \"Steps for XYZ. 1.\" or \"What are the subgoals for achieving XYZ?\" to decompose the task into smaller steps.\\n2. **Task-specific instructions**: Task-specific instructions, such as \"Write a story outline\" for writing a novel, can be used to guide the LLM in decomposing the task.\\n3. **Human inputs**: Human inputs can also be used to provide guidance on how to decompose the task.\\n\\nTwo techniques that extend traditional task decomposition are:\\n\\n1. **Chain of Thought (CoT)**: CoT is a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to \"think step by step\" to utilize more test-time computation to decompose hard tasks into smaller and simpler steps.\\n2. **Tree of Thoughts (Yao et al. 2023)**: This technique extends CoT by exploring multiple reasoning possibilities at each step, generating multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\n\\nTask decomposition is an essential component of planning in LLM-powered autonomous agent systems, enabling efficient handling of complex tasks and allowing the agent to plan ahead.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "response_prompt_template = \"\"\"\n",
    "You are an expert of world knowledge. I am going to ask you a question.\n",
    "Your response should be comprehensive and not contradicted with the following context\n",
    "if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "respones_prompt=ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "\n",
    "final_rag_chain=(\n",
    "    {\n",
    "        \"normal_context\":normal_chain,\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        \"question\":itemgetter(\"question\")\n",
    "    }\n",
    "    | respones_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "final_rag_chain.invoke({\"question\": question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
