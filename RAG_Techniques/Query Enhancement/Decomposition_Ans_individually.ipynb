{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093c917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "##indexing##\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "loader=WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "#split\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300,chunk_overlap=50)\n",
    "splits=text_splitter.split_documents(docs) #->list\n",
    "\n",
    "#embed\n",
    "embedding_model=HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-zh-v1.5\")\n",
    "vectorstore=Chroma.from_documents(documents=splits,embedding=embedding_model)\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "#ollama llm\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb7e9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three potential search queries related to \"task decomposition for LLM agents\":',\n",
       " '',\n",
       " '1. \"What is the purpose of task decomposition in large language model (LLM) training?\"',\n",
       " '2. \"How does task decomposition affect the performance of LLM agents on specific tasks, such as question answering or text classification?\"',\n",
       " '3. \"Can you explain the different types of task decomposition methods used for LLMs, such as hierarchical task decomposition and subtask decomposition?\"',\n",
       " '',\n",
       " 'These queries can be answered in isolation to provide a comprehensive understanding of task decomposition for LLM agents.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# Decomposition\n",
    "template=\"\"\"\n",
    "You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\n",
    "\"\"\"\n",
    "\n",
    "Decomposition_prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "generate_queries_decomposition= (\n",
    "    Decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x : x.split(\"\\n\"))\n",
    ")\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "\n",
    "check_Decomposition=generate_queries_decomposition.invoke({\"question\":question})\n",
    "check_Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509cabcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "prompt_rag = client.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "#Answer individually\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    sub_questions =sub_question_generator_chain.invoke({\"question\":question})\n",
    "\n",
    "    rag_result=[]\n",
    "\n",
    "    for sub_question in sub_questions:\n",
    "        retrieved_docs=retriever.invoke(sub_question)\n",
    "\n",
    "        answer = (\n",
    "             prompt_rag\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "            ).invoke({\"context\":retrieved_docs, \"question\":sub_question})\n",
    "        rag_result.append(answer)\n",
    "\n",
    "    return rag_result,sub_questions\n",
    "\n",
    "answer,questions=retrieve_and_rag(question,prompt_rag,generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf0ab0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided Q&A pairs, task decomposition for LLM agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals. This technique enables efficient handling of large tasks by allowing LLMs to think step-by-step and utilize more test-time computation.\\n\\nTask decomposition can be achieved through various methods, including:\\n\\n1. Hierarchical task decomposition: Breaking down complex tasks into smaller, more manageable subtasks, as seen in Chain of Thought (CoT) and Tree of Thoughts (Yao et al.). This method involves decomposing big tasks into multiple simpler steps.\\n2. Subtask decomposition: Identifying specific subgoals within a larger task using simple prompting or task-specific instructions.\\n\\nThe benefits of task decomposition for LLM agents include:\\n\\n* Efficient handling of complex tasks\\n* Improved planning, reflection, and memory capabilities\\n\\nTask decomposition can be done through various techniques, including:\\n\\n* Simple prompting (e.g., \"Steps for XYZ.\")\\n* Task-specific instructions\\n* Human inputs\\n\\nOverall, task decomposition is a crucial technique for LLM agents to tackle complex tasks efficiently and effectively.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_QA(answers,questions):\n",
    "    format_string=\"\"\n",
    "    for i ,(question,answer) in enumerate (zip(questions,answers) , start=1):\n",
    "        format_string=format_string+f\"Question {i}: {question} \\n answer{i}:{answer} \\n\\n\"\n",
    "    return format_string.strip()\n",
    "\n",
    "context=format_QA(answer,questions)\n",
    "\n",
    "template=\"\"\"\n",
    "Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain=(\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
