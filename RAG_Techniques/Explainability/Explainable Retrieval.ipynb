{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8069,
     "status": "ok",
     "timestamp": 1770808003081,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "v48l5OYmSYXZ",
    "outputId": "302227e8-050f-4593-b7c8-b95bdc4c9fca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading pypdf-6.7.0-py3-none-any.whl (330 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.6/330.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: pypdf, ollama, langchain-text-splitters, langchain-ollama, langchain-huggingface, langchain-community, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2 langchain-community-0.4.1 langchain-huggingface-1.2.0 langchain-ollama-1.0.1 langchain-text-splitters-1.1.0 ollama-0.6.1 pypdf-6.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters  pypdf  langchain-huggingface sentence-transformers faiss-cpu ollama langchain-ollama   --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51159,
     "status": "ok",
     "timestamp": 1770808054247,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "iaeLMrHZT5Yv",
    "outputId": "c2d80835-e1b3-4a92-b401-e5ce20ebcf42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 第一個 Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# 注意：這裡刻意不寫 torch，直接用 Colab 內建的，就不會打架也不會慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "733610489fc24ce3917cc76dec095869",
      "eecd6776047846fca884f8367dca5162",
      "90319765339e42c684f9f025aa46923c",
      "d279f26b70f34f5eb8feb7b2a0ec2eb8",
      "d52bf2527cec47b2b07c5af2ceb12133",
      "0bf044f81d6f4138bac2685a4d348ab7",
      "a4db575335da46418188e64ce7fffd2a",
      "1bbf2cb9cc1c45669c1d324dcd20c57d",
      "9e0e1e3324ff41b99e5823c1bb549814",
      "b8e11c09f7904522a8cdb724cc927025",
      "964e530d07ec43a7bc62e8c4b137c574",
      "712ab1fe2ae847e4aa90d0e7b13f01a4",
      "613392ec10e6481092deda0ddbde871f",
      "86ad48accb034c2ba020f913ba56452b",
      "8edf5ef4a55f42ab8a1f8d42ebf4b740",
      "8337610d4a2f49819831214d217ba9c5",
      "7dad8685200a432db69d29150c5b2117",
      "7450ec5557e44b87adf623481a41c5d4",
      "cc1b0cb99c024e04903c8a82c631917d",
      "72d07219687a409ba5ba0ab522a37545",
      "3483d862ccff4bc187059bcd924de642",
      "216f93f3412c41ac96f7bce9bfa5b70b",
      "d58331df035945ee9040140772a04f47",
      "4b41e40245784a578b5536a2082d026f",
      "c38822d1b30e4890a923162b0557fbcf",
      "4ba49327a89041769a31a7048e38adb9",
      "483747e231df4341840b369c5b165c8c",
      "acc76a5c76d748f5a89d43e0101cc323",
      "51cf6ce622a84838b4fe99303fa61647",
      "cd89e84863b44ec2ba8d40eaf856343a",
      "5fa13b15d93947f5a90e4c20bad339f8",
      "710321a77b334df880309e855c8e590b",
      "a6a06ca7aa0743f6995dff33c04d0676",
      "3007484fa6294f4cbdf3bbb1e3d0b056",
      "9d7291c5d9bf4e0d81a0d412b450aa67",
      "223c190be4ca449097cfd0181cf22b2d",
      "b37781fbffa342e696e0a9dc7d02e6cf",
      "e2f696cbe7904bbca17088fe7fad22c1",
      "34c90678a8294a9bb98dd37e1adc8b1a",
      "26c5a909ce47464fbd5596fa302b7918",
      "f9f1d87654f74ee9a4f6ed7d620176a6",
      "5edf93ed38834aacb09541dbbdeabdda",
      "f576334579db4a2eb11e3feed8edfc18",
      "0030970cd32547bc82f173a140ba36f2",
      "ab00cb0ea70d455b935478146f8c6640",
      "81dffdbb184340b7adf148aa47089e31",
      "db9e37e5ecf64096b9aa61593d23b6dc",
      "23fc8c90cb9247ff85229aa360e0184e",
      "b37ee84567f549aabb4f54b72a6cdd05",
      "1dcae54ea12841a582c1c0080cdb87dc",
      "17e0971388d045f9bb3473ff4a39a363",
      "8240b8a9b51b4266acd281ed43c3233a",
      "f0066768839d40859091de75daa2db0f",
      "bd09f847c33641028b9b0d94089ed463",
      "556c4d78b11d4b4ea21e5ea9d3f0e2af",
      "88e740463e19436f9e0075f213cf9128",
      "fa8588903ce541029905e77bbfce4aea",
      "b34906edbfa14bbfa2d01e884d3db5d1",
      "9dd37d2af0db43f3bedddab2b0068656",
      "d8b2fe4051044e408bece69f3fc2fc3d",
      "649f03fc7269497a9935986448fceb17",
      "7717defbc2094901a01070ef6e882249",
      "82180d8eb28f4570a8c07ed352abe649",
      "80f57e51f308429fbf6105c46947e2d9",
      "b6a245d414394cc0a0777bb427440def",
      "e79c593e84a94c6b8f8dd47e350cc3de",
      "b2139fe9cc464d9384b680f54d41bc49",
      "81cebf26609a41a990418e02d6d12005",
      "1f19d5ef4d1d4268a434e2486cd37d65",
      "1e59e2feb58749cb94081df0a58906a9",
      "d4501f377f2649dd9300cd2a9b14c8ab",
      "a71b73d0a291464ebdc7d7dfcfe129d8",
      "f81661035a0342b4b5645e6cfdd426a3",
      "4639890f78754fa0a2216c16464813a1",
      "00f81e65b6f944ad8e10dd774c3cfda2",
      "e2c45e2c3420442fa90aa2c116956388",
      "44a89885f5f44cb08a0df5e2bf2bcd50",
      "df50c2a63af44f13a90d50fffeda0c9a",
      "6a3dffac363e47afaaf3c04c36d0ac7b",
      "bfc8d2ab5966488692a62d0c22b58bfe",
      "b1641c78a5a147f19139e8aaf1214c2a",
      "8bad9734daba49fa9e5741868a379ec2",
      "bff138ddd6f248eca85d2e422c954941",
      "8778f48e91eb40149c5e9f7957ffacae",
      "9a7f1faaa91f43ee85a3059b0c494a26",
      "308c66d73f1640e18ccfb35eac733976",
      "a954a87208de420cb31fc4b20f974a99",
      "87ceb2060e104750a5264bf49a0cf2e8",
      "65fe41dcce2543c680ae110be0e10ad9",
      "540d836ae8d84e5ea4aed6640485ffae",
      "7689b00337494afea36a37d470ff523a",
      "8790169ff3e845b8986574a955807302",
      "e201b3f54e7d49b1aa9b1a88713071ac",
      "39cac9406ed44fb788e0e5464260b5d3",
      "e12d323cf5154ce5bfd62c2d37207d56",
      "ac4adba7b23f4327a84d39be8bfc9e59",
      "0f22af5fb0694257874fab9024719fa4",
      "a174b82805cd4008bd03e07f3d5cead4",
      "6da2d460dc7a4d4aa48055327cd0e41f",
      "7d649e5b8f0b4c63ba7e9c84d59aa5e9",
      "5430094ecb2942858ee63018ca1e5746",
      "6e7937c0acaa4b4c9e5cca4c09c0a35d",
      "bba01e3dd24546b78f4b428cd9e76cf0",
      "4397dd369193437eb59cde97d3053e60",
      "3ffe82a229734e638c4d62cae088e774",
      "2aea519987674f18a74c1da2bd320332",
      "33d862163bda47dcb4a95cf910e5e89a",
      "1a58b53458bc4cc1911f6fcbaa163bde",
      "e45dec70d9774085ba989b0dc226a3d4",
      "f1a28f00ffeb48e581b270194137d068"
     ]
    },
    "executionInfo": {
     "elapsed": 192294,
     "status": "ok",
     "timestamp": 1770808268283,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "3aZ5t4GeT6dk",
    "outputId": "ba6137c6-1669-412c-8783-19ff1dd152a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "733610489fc24ce3917cc76dec095869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "712ab1fe2ae847e4aa90d0e7b13f01a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58331df035945ee9040140772a04f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3007484fa6294f4cbdf3bbb1e3d0b056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab00cb0ea70d455b935478146f8c6640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e740463e19436f9e0075f213cf9128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2139fe9cc464d9384b680f54d41bc49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df50c2a63af44f13a90d50fffeda0c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65fe41dcce2543c680ae110be0e10ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d649e5b8f0b4c63ba7e9c84d59aa5e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'repetition_penalty', 'temperature', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 確保 pipeline 正常運作的常見修復\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# ==========================================\n",
    "# 建立原生 Transformers Pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,      # 決定回答長度\n",
    "    temperature=0.2,         # RAG 建議設低一點，減少幻覺\n",
    "    repetition_penalty=1.1,  # 避免鬼打牆\n",
    "    return_full_text=False   # 重要：設為 False 只回傳生成的部分\n",
    ")\n",
    "\n",
    "# . 轉為 LangChain 的 LLM 物件\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# (關鍵) 轉為 ChatModel 物件\n",
    "# ChatHuggingFace 會自動處理 prompt template (User/System/Assistant 格式)\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "fe7fbbcb4e844173b5990fa2a68c6387",
      "b6fe98a32dd247efa98b1af12058104a",
      "b839eeaa38f04a1b909753625ff22a08",
      "efdfa348cfde45e782dba39e05113bfb",
      "16f883ac8a824b9a89f7f58bba1dfa4a",
      "a1b1dce614f74f0d80156aeb0d453e1b",
      "834d4b8f9c3b4aa99f9b89d9e34ca066",
      "08ec789bd0d1405e81853e3a54a75b0c",
      "ed86b7c9998947819af439acfe2029e2",
      "e71d4f6f5bc049d9854b20e7c99b4652",
      "47dc1eeadf5f4ab99f7a74121a59d763",
      "bff54b2aa84a4d6fadb96646836b17e1",
      "e1c37e35867f4a7f8ad5bf252a14c88f",
      "71dcaae4fe334b7bac6257d2ad165a55",
      "fb03b6da45cf44abaa8f23fc101f32c6",
      "1e5013e74a1846f19fe6d17062780e1e",
      "2f29a3a0704a4ab286adb4e678bb72f6",
      "0d76129c921d4dd2bfdfc476babad965",
      "366e96cbeb7a4751a5382bd0b84bc6be",
      "e5de520159a749c49ea8c1a54cc1024f",
      "2f2a85a664594facaa88cd934b114a99",
      "c322869aacc8419997ddfd5516fb0190",
      "c1e500dbf7fc4d8ca4f6db23f11e0d4d",
      "07ffd19dea3443c6b96b2ac46888b29f",
      "b4b5b563c0f149519c3159762656d3df",
      "02823ffcde26480da6734e89bee6a8c4",
      "ad849af35def4b2fa9e757b7d4c20994",
      "126837411a1d4b0dba9ceebe9d644db2",
      "63115ee1dd5e4b86a63a8d5bd30c938b",
      "979533b21b254238b1ecd02b9bf139ab",
      "f2e8f46419274e45889b94aec916c31f",
      "a8caf58ae06f4fa4ade4066d1fbdb753",
      "ce8de7bbd0224c3d99e86e5355dee109",
      "42e686357e6f4d5db5791fdb69fccd95",
      "08f7140a147d481c8de275f06ac86b2b",
      "f2dc001e3eb841a6bf27dcd2ca176d5b",
      "78f4b199e35b4e32890cae15eb0995fc",
      "070d9fa7755e4ae29dcf9c21539d72d5",
      "1a8b36a1349d4f1da548da2d07a06632",
      "ae3b1d2122ac49d79a9664499bda15f5",
      "4e727f2b08194eccb55c0f325e4c9a19",
      "a9361d78eac84a05b2a32854cdd3db55",
      "459406850d054d3d89820ea5e8da7fb5",
      "dd72994b63a347c4b1532b0368ff734c",
      "8b3bbe37f3d24ac993d713b308733bed",
      "d339a71548ab4be385c3a9e93b2d533e",
      "449007148cdb4ccf944c0129a2e31c8e",
      "1fe1fda4445d4303b96f3dbae377b42e",
      "a107c589fe364a57984f065a09f0145e",
      "5eaef0dc22524d43b956abae7341f038",
      "cc67d6bf52164545b3f581e28d3726b4",
      "dd21cb47fc49426383a6651f50b429c0",
      "b662804b616c455cb68884dd04269f94",
      "b41cbf1b75124745b450390b41d4e104",
      "f05e9caf6cc8484e974128e1ad90e996",
      "60906a7ca82f436b92d7d4bfab48626e",
      "6a3295f2984e4e3fb3659781f3b79798",
      "5a974a2bfa844298ad769938e5ba53ca",
      "9731dc261c0441b490a461e9145e4d96",
      "9492ec29b39946268a50aee2cf523db7",
      "66c61de02bc749e9bb43a36722a91b6d",
      "f8082fca46164844ab6649bf70add28d",
      "9a0304738c7444129a461b0387ab3fe1",
      "f8a0c19d1e0d4fd0b198f294d5fffcb6",
      "1689235359e9468a9b71b91afc547bdc",
      "425a10fb19e64e89ba27b70c0ee9fc07",
      "1f134e88db384ff6972dd301f2b1a101",
      "e1cf039259574766b80d1dd8b11fa19a",
      "06a9e839a7434f76b7dac6c1f248a4bf",
      "cbdb226df72647dd8646dc91d0725777",
      "066ebcca033b4306957b8e44063711a2",
      "64192d84e14d4ba2bb9888a6f5f840de",
      "7e0f14479a0c44d2a84a532bf15f784b",
      "1927cbca02554d3baa79dcf13d135371",
      "0ec2aa987e714d6c9321f87098008561",
      "749e8a7eb02b45dcaabffdcbac3358ba",
      "61977870706e40acbb2b786e6eb2022c",
      "2f0ff2dbcb174b47b8e43bb05ef8266f",
      "bb355851e7904f39a146c703a2c40cf4",
      "e05ac36a063946c296d481946fd9954b",
      "28786affb1ac4ce89da1253da1e11a97",
      "d53d2fa2666f45489e9503531d26e76b",
      "3adb412fe8f2479c93b36c0d40fe0d68",
      "75c51b59ecc840fe8c9d21feb9ef229c",
      "bb81c336ed764abbb157707a27d63a03",
      "eafaf604a27a40c88a5fa57e5136f068",
      "913ee069b3664822b50e01d666d53977",
      "d1f4024edf214fd4aea3f9c5a19d6ea1",
      "49674ce39e4842cab7612d53c5ee3a66",
      "929e454eade343869c3cc217ae67d24c",
      "99f1977356a648159b0c30ee412ef1cd",
      "42e6cef1ed9445aba9c86069be27b02e",
      "9e696332379f4216ac2771720ea15a5f",
      "e6cb81ffe8a64c019ec124d207dd9ac0",
      "0798f82c7a0f46c688a2654033fae26b",
      "d84819f26ed148a7af7d99315628e969",
      "861f2f03e6cc49349b9c9c9f5efe0924",
      "2956abc99c3645b9bf9264ba4ec0196b",
      "e0746d351f534e74b47eae218b2db846",
      "0d3d80e9bfa24b868070f14de6da5858",
      "d83eb0bd13c94b8cac161e82484b9e45",
      "303f112a6ff44a96a32fc829d9806f43",
      "380ca4dd8b1a4cf1b2592c39dad9a228",
      "62fb6926b8104688a6b266533166a80c",
      "6d4f634ca94c4adc86b2fcacb17cb320",
      "96ba4310e1bb41a5b79ed064683bef36",
      "30fb181a8c1f485e9c268f8fc01be15d",
      "6676a2ac59fb4aa6a61b53f0087e170d",
      "1d09cdba7651460fa874d96cf457b8bf",
      "22730083f9354b6cba05a2e85f38e1b7",
      "cb5742dc341745d89ac742af8ff5f4b0",
      "0f70cb6e6c174ea89607cfaff98f27c1",
      "66315ca69351447c8c51bda2e0215dbf",
      "2dff8cd57b7a4c8db17bab0abc389e14",
      "3390e5ca3cc74309895edd9c639a9b44",
      "16ba3aa1a4b94c829c65ce4cf81bce5b",
      "a61e48a647974746b73b83c7e6743082",
      "f1b2800a051740f1b2b009a8f260a891",
      "5c4be149573b4b959cf47da3d34bd023",
      "cddb852c9e2f4c6a8dc7c7f94422b7ae",
      "4199ce40b91741cf8ff23f33be416567"
     ]
    },
    "executionInfo": {
     "elapsed": 29331,
     "status": "ok",
     "timestamp": 1770808297619,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Xwm212pPT_KL",
    "outputId": "f34b89a2-5ef5-41d4-e78b-4beaf499accb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7fbbcb4e844173b5990fa2a68c6387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff54b2aa84a4d6fadb96646836b17e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/215 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e500dbf7fc4d8ca4f6db23f11e0d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e686357e6f4d5db5791fdb69fccd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3bbe37f3d24ac993d713b308733bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60906a7ca82f436b92d7d4bfab48626e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f134e88db384ff6972dd301f2b1a101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0ff2dbcb174b47b8e43bb05ef8266f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49674ce39e4842cab7612d53c5ee3a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3d80e9bfa24b868070f14de6da5858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5742dc341745d89ac742af8ff5f4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/313 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVEt5IDtcblr"
   },
   "source": [
    "##Define the explainable retriever class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1770809035214,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "F72wU2Z6UDCa"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "class ExplainableRetriever:\n",
    "    def __init__(self,texts,embedding,chat_moedel):\n",
    "        self.embedding=embedding\n",
    "        self.llm=chat_moedel\n",
    "        self.vectorstore=FAISS.from_texts(texts,self.embedding)\n",
    "        self.retriever=self.vectorstore.as_retriever()\n",
    "\n",
    "        explain_prompt=PromptTemplate(\n",
    "            input_variables=[\"query\",\"context\"],\n",
    "            template=\"\"\"\n",
    "            Analyze the relationship between the following query and the retrieved context.\n",
    "            Explain why this context is relevant to the query and how it might help answer the query.\n",
    "\n",
    "            Query: {query}\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Explanation:\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "        self.explain_chain=explain_prompt | self.llm\n",
    "\n",
    "    def retrieve_and_explian(self,query):\n",
    "        docs=self.retriever.invoke(query,k=3)\n",
    "        explained_result=[]\n",
    "\n",
    "        for doc in docs:\n",
    "            input_data={\"query\":query,\"context\":doc.page_content}\n",
    "            explanation=self.explain_chain.invoke(input_data)\n",
    "            explained_result.append({\n",
    "                \"content\":doc.page_content,\n",
    "                \"explanation\":explanation\n",
    "            })\n",
    "\n",
    "        return explained_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJFI77ceUA"
   },
   "source": [
    "##Create a mock example and explainable retriever instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1770809036353,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "v9Dpe1nsYzv9"
   },
   "outputs": [],
   "source": [
    "# Usage\n",
    "texts = [\n",
    "    \"The sky is blue because of the way sunlight interacts with the atmosphere.\",\n",
    "    \"Photosynthesis is the process by which plants use sunlight to produce energy.\",\n",
    "    \"Global warming is caused by the increase of greenhouse gases in Earth's atmosphere.\"\n",
    "]\n",
    "\n",
    "explainable_retriever = ExplainableRetriever(texts,embedding_model,chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRvN749QcgJb"
   },
   "source": [
    "##Show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31767,
     "status": "ok",
     "timestamp": 1770809068666,
     "user": {
      "displayName": "314657023柏瑞",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "o4JYd3ArZNl0",
    "outputId": "4592828f-2efc-48ae-8dc6-8cb034907169"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      "Content: The sky is blue because of the way sunlight interacts with the atmosphere.\n",
      "Explanation: content='The context is highly relevant to the query \"Why is the sky blue?\" because it directly addresses the scientific explanation behind the phenomenon. The statement \"The sky is blue because of the way sunlight interacts with the atmosphere\" provides a foundational, accurate principle that explains the observed color of the sky.\\n\\nThis context helps answer the query by introducing the key concept—sunlight scattering in the atmosphere—which is essential for understanding the actual mechanism (Rayleigh scattering). While the context does not go into detailed physics or wavelengths, it correctly identifies the core cause and sets the stage for further exploration. Therefore, it serves as an appropriate and concise starting point for answering' additional_kwargs={} response_metadata={} id='lc_run--019c4c71-947d-7980-b08c-4c19eba1fbe1-0' tool_calls=[] invalid_tool_calls=[]\n",
      "\n",
      "Result 2:\n",
      "Content: Global warming is caused by the increase of greenhouse gases in Earth's atmosphere.\n",
      "Explanation: content='The provided context — \"Global warming is caused by the increase of greenhouse gases in Earth\\'s atmosphere\" — is **not relevant** to the query \"Why is the sky blue?\"\\n\\nExplanation:  \\nThe query asks for an explanation of a phenomenon related to light and atmospheric physics (the color of the sky), specifically involving Rayleigh scattering, where shorter wavelengths of light (like blue) are scattered more than longer ones (like red) by molecules in the atmosphere. This scientific principle directly explains why the sky appears blue during the day.\\n\\nIn contrast, the given context discusses global warming and greenhouse gases, which relates to climate change and long-term temperature' additional_kwargs={} response_metadata={} id='lc_run--019c4c71-b998-7b50-8a0c-ac00cad8d322-0' tool_calls=[] invalid_tool_calls=[]\n",
      "\n",
      "Result 3:\n",
      "Content: Photosynthesis is the process by which plants use sunlight to produce energy.\n",
      "Explanation: content='The provided context — \"Photosynthesis is the process by which plants use sunlight to produce energy\" — is **not relevant** to the query \"Why is the sky blue?\"\\n\\nExplanation:  \\nThe query asks for an explanation of a phenomenon related to atmospheric physics and light scattering (specifically, Rayleigh scattering), where shorter wavelengths of light (like blue) are scattered more than longer ones (like red), giving the sky its blue appearance. The given context discusses photosynthesis, which is a biological process in plants involving the conversion of sunlight into chemical energy. This topic has no connection to the color of the sky or the behavior of light in' additional_kwargs={} response_metadata={} id='lc_run--019c4c71-e24f-7823-9dbf-474de640c30a-0' tool_calls=[] invalid_tool_calls=[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Why is the sky blue?\"\n",
    "results = explainable_retriever.retrieve_and_explian(query)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Content: {result['content']}\")\n",
    "    print(f\"Explanation: {result['explanation']}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNNRcDjDJpjHBclbk+rviTu",
   "gpuType": "T4",
   "mount_file_id": "1c2orswNTwKJURKHzorWCmWN6xBe1_FGm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
