{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d468e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a692475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_tab_with_space(texts):\n",
    "\n",
    "    for text in texts:\n",
    "        text.page_content=text.page_content.replace(\"\\t\",\" \")\n",
    "    return texts\n",
    "\n",
    "\n",
    "def encode_pdf(path,token_size=300,token_overlap=50):\n",
    "    loader=PyPDFLoader(path)\n",
    "    docs=loader.load()\n",
    "\n",
    "    text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=token_size,chunk_overlap=token_overlap)\n",
    "    texts=text_splitter.split_documents(docs)\n",
    "\n",
    "    clean_texts=replace_tab_with_space(texts)\n",
    "\n",
    "    embedding_model=OllamaEmbeddings(model=\"qwen3-embedding:0.6b\")\n",
    "\n",
    "    vectorstore=Chroma.from_documents(embedding=embedding_model,documents=clean_texts,collection_name=\"qwen3_embedding_1024\" )\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "def retriever_context_per_question(question,query_retriever):\n",
    "\n",
    "    docs=query_retriever.invoke(question,k=2)\n",
    "\n",
    "    context=[doc.page_content for doc in docs]\n",
    "\n",
    "    return context,docs\n",
    "\n",
    "def show_context(context):\n",
    "    for i,c in enumerate(context):\n",
    "        print(f\"context{i+1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e123a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=r\"C:\\Users\\user\\Desktop\\暑假語言模型\\RAG資料集\\2401.15884v3.pdf\"\n",
    "\n",
    "vectorstore=encode_pdf(path)\n",
    "\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "question=\"What the CRAG proposed to improve \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f42ef6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context1:\n",
      "implemented into RAG (Lewis et al., 2020) and\n",
      "Self-RAG (Asai et al., 2024) for demonstrating its\n",
      "adaptability to RAG-based approaches. Results on\n",
      "four datasets of PopQA (Mallen et al., 2023), Biog-\n",
      "raphy (Min et al., 2023), Pub Health (Zhang et al.,\n",
      "2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n",
      "2021) show that CRAG can significantly improve\n",
      "the performance of standard RAG and state-of-the-\n",
      "art Self-RAG, demonstrating its generalizability\n",
      "across both short- and long-form generation tasks.\n",
      "To facilitate others to reproduce our results, we will\n",
      "publish all source code later.\n",
      "In summary, our contributions in this paper are\n",
      "three-fold: 1) This paper studies the scenarios\n",
      "where the retriever returns inaccurate results and,\n",
      "to the best of our knowledge, makes the first\n",
      "attempt to design corrective strategies for RAG to\n",
      "improve its robustness. 2) A plug-and-play method\n",
      "named CRAG is proposed to improve the ability of\n",
      "automatic self-correction and efficient utilization\n",
      "of retrieved documents. 3) Experimental results\n",
      "extensively demonstrate CRAG ’s adaptability to\n",
      "RAG-based approaches and its generalizability\n",
      "\n",
      "\n",
      "context2:\n",
      "instance in practice, as detailed in Table 6. The\n",
      "findings indicate that the self-correction mecha-\n",
      "nism incurs only modest computational overhead\n",
      "while significantly enhancing performance, thereby\n",
      "validating its lightweight nature.\n",
      "6 Conclusion & Limitation\n",
      "This paper studies the problem where RAG-based\n",
      "approaches are challenged if retrieval goes wrong,\n",
      "thereby exposing inaccurate and misleading knowl-\n",
      "edge to generative LMs. Corrective Retrieval\n",
      "Augmented Generation is proposed to improve the\n",
      "robustness of generation. Essentially, a lightweight\n",
      "retrieval evaluator is to estimate and trigger three\n",
      "knowledge retrieval actions discriminately. With\n",
      "the further leverage of web search and optimized\n",
      "knowledge utilization, CRAG has significantly\n",
      "improved the ability of automatic self-correction\n",
      "and efficient utilization of retrieved documents.\n",
      "Experiments extensively demonstrate its adaptabil-\n",
      "ity to RAG-based approaches as well as general-\n",
      "izability across short- and long-form generation\n",
      "tasks. While we primarily proposed to improve the\n",
      "RAG framework from a corrective perspective and\n",
      "CRAG can be seamlessly coupled with various\n",
      "RAG-based approaches, fine-tuning an external\n",
      "retrieval evaluator is inevitable. How to eliminate\n",
      "this external evaluator and equip LLMs with better\n",
      "retrieval evaluation capabilities will be our future\n",
      "work.\n",
      "References\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context,docs=retriever_context_per_question(question,retriever)\n",
    "show_context(context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca70e1",
   "metadata": {},
   "source": [
    "##### 讓llm自己判斷檢索文章和內容有沒有相關"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c08cfa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel,Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "class GradDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score : str = Field(\n",
    "        ...,\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)\n",
    "structured_llm_grader=llm.with_structured_output(GradDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
    "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#structured_llm_grader內建已經有output parser\n",
    "grade_chain=grade_prompt | structured_llm_grader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6691cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "implemented into RAG (Lewis et al., 2020) and\n",
      "Self-RAG (Asai et al., 2024) for demonstrating its\n",
      "adaptability to RAG-based approaches. Results on\n",
      "four datasets of PopQA (Mallen et al., 2023), Biog-\n",
      "raphy (Min et al., 2023), Pub Health (Zhang et al.,\n",
      "2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n",
      "2021) show that CRAG can significantly improve\n",
      "the performance of standard RAG and state-of-the-\n",
      "art Self-RAG, demonstrating its generalizability\n",
      "across both short- and long-form generation tasks.\n",
      "To facilitate others to reproduce our results, we will\n",
      "publish all source code later.\n",
      "In summary, our contributions in this paper are\n",
      "three-fold: 1) This paper studies the scenarios\n",
      "where the retriever returns inaccurate results and,\n",
      "to the best of our knowledge, makes the first\n",
      "attempt to design corrective strategies for RAG to\n",
      "improve its robustness. 2) A plug-and-play method\n",
      "named CRAG is proposed to improve the ability of\n",
      "automatic self-correction and efficient utilization\n",
      "of retrieved documents. 3) Experimental results\n",
      "extensively demonstrate CRAG ’s adaptability to\n",
      "RAG-based approaches and its generalizability \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n",
      "instance in practice, as detailed in Table 6. The\n",
      "findings indicate that the self-correction mecha-\n",
      "nism incurs only modest computational overhead\n",
      "while significantly enhancing performance, thereby\n",
      "validating its lightweight nature.\n",
      "6 Conclusion & Limitation\n",
      "This paper studies the problem where RAG-based\n",
      "approaches are challenged if retrieval goes wrong,\n",
      "thereby exposing inaccurate and misleading knowl-\n",
      "edge to generative LMs. Corrective Retrieval\n",
      "Augmented Generation is proposed to improve the\n",
      "robustness of generation. Essentially, a lightweight\n",
      "retrieval evaluator is to estimate and trigger three\n",
      "knowledge retrieval actions discriminately. With\n",
      "the further leverage of web search and optimized\n",
      "knowledge utilization, CRAG has significantly\n",
      "improved the ability of automatic self-correction\n",
      "and efficient utilization of retrieved documents.\n",
      "Experiments extensively demonstrate its adaptabil-\n",
      "ity to RAG-based approaches as well as general-\n",
      "izability across short- and long-form generation\n",
      "tasks. While we primarily proposed to improve the\n",
      "RAG framework from a corrective perspective and\n",
      "CRAG can be seamlessly coupled with various\n",
      "RAG-based approaches, fine-tuning an external\n",
      "retrieval evaluator is inevitable. How to eliminate\n",
      "this external evaluator and equip LLMs with better\n",
      "retrieval evaluation capabilities will be our future\n",
      "work.\n",
      "References \n",
      " --------------------------------------------------\n",
      "binary_score='yes' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Filter out the non-relevant docs\n",
    "docs_to_use=[]\n",
    "for doc in docs :\n",
    "    print(doc.page_content, \"\\n\" ,\"-\"*50)\n",
    "    res=grade_chain.invoke({\"question\":question,\"document\":doc.page_content})\n",
    "    print(res,\"\\n\")\n",
    "    if res.binary_score == \"yes\":\n",
    "        docs_to_use.append(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee28edfb",
   "metadata": {},
   "source": [
    "##### 用llm認為相關的文章回答答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd7a191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRAG (Corrective Retrieval Augmented Generation) is proposed to improve the robustness of generation in RAG-based approaches. Specifically, it aims to address the problem where retrieval goes wrong and exposes inaccurate knowledge to generative LMs. By estimating and triggering three knowledge retrieval actions discriminately, CRAG enhances the ability of automatic self-correction and efficient utilization of retrieved documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "system = \"\"\"You are an assistant for question-answering tasks.\n",
    "Answer the question based upon your knowledge.\n",
    "Use three-to-six sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "#<docs>是標籤讓讓llm更好理解範圍\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n \"\n",
    "         \"User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)\n",
    "\n",
    "#post-processing\n",
    "def forma_docs(docs):\n",
    "    return \"\\n\".join(\n",
    "        f\"<doc{i+1}>: \\n\"\n",
    "        f\"content:{doc.page_content} \\n\"\n",
    "        f\"</doc{i+1}> \\n\"\n",
    "        for i,doc in enumerate (docs)\n",
    "    )\n",
    "\n",
    "final_chain=prompt | llm | StrOutputParser()\n",
    "\n",
    "generation=final_chain.invoke({\"question\":question,\"documents\":forma_docs(docs_to_use)})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d90145",
   "metadata": {},
   "source": [
    "#### Check for Hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5e46e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in 'generation' answer.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)\n",
    "structured_llm_grader=llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system=\"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in a set of retrieved facts. \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in the set of facts.\n",
    "\"\"\"\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",system),\n",
    "    (\"human\",\"Set of fact <fact>{documents}</fact> \\n\\n\"\n",
    "    \" LLm generation :<generation>{generation}</generation>\")\n",
    "])\n",
    "\n",
    "hallucination_grader_chain=prompt | structured_llm_grader\n",
    "\n",
    "respond=hallucination_grader_chain.invoke({\"documents\":forma_docs(docs_to_use),\"generation\":generation})\n",
    "print(respond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b8342b",
   "metadata": {},
   "source": [
    "##### Highlight used docs 標出回答用的是那些資訊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9897bd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class HighlightDocuments(BaseModel):\n",
    "    \"\"\"Return the specific part of a document used for answering the question.\"\"\"\n",
    "\n",
    "    id: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of id of docs used to answers the question\"\n",
    "    )\n",
    "\n",
    "    title: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of titles used to answers the question\"\n",
    "    )\n",
    "\n",
    "    source: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of sources used to answers the question\"\n",
    "    )\n",
    "\n",
    "    segment: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"List of direct segements from used documents that answers the question\"\n",
    "    )\n",
    "# LLm\n",
    "llm=ChatOllama(model=\"qwen3:4b-instruct-2507-q8_0\",temperature=0)\n",
    "\n",
    "#Parser\n",
    "parser=PydanticOutputParser(pydantic_object=HighlightDocuments)\n",
    "\n",
    "#prompt\n",
    "template = \"\"\"You are an advanced assistant for document search and retrieval. You are provided with the following:\n",
    "1. A question.\n",
    "2. A generated answer based on the question.\n",
    "3. A set of documents that were referenced in generating the answer.\n",
    "\n",
    "Your task is to identify and extract the exact inline segments from the provided documents that directly correspond to the content used to\n",
    "generate the given answer. The extracted segments must be verbatim snippets from the documents, ensuring a word-for-word match with the text\n",
    "in the provided documents.\n",
    "\n",
    "Ensure that:\n",
    "- (Important) Each segment is an exact match to a part of the document and is fully contained within the document text.\n",
    "- The relevance of each segment to the generated answer is clear and directly supports the answer provided.\n",
    "- (Important) If you didn't used the specific document don't mention it.\n",
    "\n",
    "Used documents: <docs>{documents}</docs> \\n\\n User question: <question>{question}</question> \\n\\n Generated answer: <answer>{generation}</answer>\n",
    "\n",
    "<format_instruction>\n",
    "{format_instructions}\n",
    "</format_instruction>\n",
    "\"\"\"\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=[\"question\",\"documents\",\"generation\"],\n",
    "    template=template,\n",
    "    partial_variables={\"format_instructions\":parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "highlight_chain=prompt | llm | parser\n",
    "\n",
    "respond=highlight_chain.invoke({\n",
    "    \"question\":question,\"documents\":forma_docs(docs_to_use),\"generation\":generation\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6507e1a",
   "metadata": {},
   "source": [
    "Pydantic 的特性是：當你建立一個這個模型的物件時，它會自動把所有定義的欄位當作 物件屬性，並且提供驗證、類型檢查等功能。<br>\n",
    "highlight.id       # 對應 id 欄位<br>\n",
    "highlight.title    # 對應 title 欄位<br>\n",
    "highlight.source   # 對應 source 欄位<br>\n",
    "highlight.segment  # 對應 segment 欄位<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f386754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID : doc1 \n",
      "title : CRAG proposed to improve robustness in RAG-based approaches \n",
      "source : This paper studies the problem where RAG-based approaches are challenged if retrieval goes wrong, thereby exposing inaccurate and misleading knowledge to generative LMs. Corrective Retrieval Augmented Generation is proposed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. \n",
      "Text segment : CRAG (Corrective Retrieval Augmented Generation) is proposed to improve the robustness of generation in RAG-based approaches. Specifically, it aims to address the problem where retrieval goes wrong and exposes inaccurate knowledge to generative LMs. By estimating and triggering three knowledge retrieval actions discriminately, CRAG enhances the ability of automatic self-correction and efficient utilization of retrieved documents.\n",
      "ID : doc2 \n",
      "title : CRAG proposed to improve robustness in RAG-based approaches \n",
      "source : This paper studies the problem where RAG-based approaches are challenged if retrieval goes wrong, thereby exposing inaccurate and misleading knowledge to generative LMs. Corrective Retrieval Augmented Generation is proposed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents. \n",
      "Text segment : Corrective Retrieval Augmented Generation is proposed to improve the robustness of generation. Essentially, a lightweight retrieval evaluator is to estimate and trigger three knowledge retrieval actions discriminately. With the further leverage of web search and optimized knowledge utilization, CRAG has significantly improved the ability of automatic self-correction and efficient utilization of retrieved documents.\n"
     ]
    }
   ],
   "source": [
    "for id,title,source,segment in zip(respond.id,respond.title,respond.source,respond.segment):\n",
    "    print(f\"ID : {id} \\ntitle : {title} \\nsource : {source} \\nText segment : {segment}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
