{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed!\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"DeepEval.ipynb\"\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "nbformat.write(nb, notebook_path)\n",
    "\n",
    "print(\"Fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9118,
     "status": "ok",
     "timestamp": 1770725737656,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "x5gH0ZTzbzFG",
    "outputId": "8e45a1c9-f5e6-4022-80f3-8487da861b89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
      "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.7.0)\n",
      "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.7)\n",
      "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting posthog\n",
      "  Downloading posthog-7.8.5-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: deepeval in /usr/local/lib/python3.12/dist-packages (3.8.4)\n",
      "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Downloading posthog-7.8.5-py3-none-any.whl (194 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: posthog, portalocker\n",
      "Successfully installed portalocker-3.2.0 posthog-7.8.5\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters pypdf pymupdf langchain-huggingface sentence-transformers langchain-experimental portalocker posthog deepeval  --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "5ce125398f9047238392570cf29e7360",
      "5ec01acc64bf43a6a273207f4fa65155",
      "432ae83f76fa40c18d414f52a69fae8a",
      "3799c7d213854c95846e7a50c1767723",
      "8377bdb16180460bba88e1015c9e4aee",
      "aa722e18cb004e69b3ef6a0a4d79c742",
      "060f15ce44c6458f9151a2af450d78f6",
      "0062efc3594541609f212575879500a7",
      "262c38645dc947239738ce8ee59bbaf7",
      "79a486c0a35c4e6aa6ca2a403fae4182",
      "d769e6f0683748dc9794115a3116d903"
     ]
    },
    "executionInfo": {
     "elapsed": 45394,
     "status": "ok",
     "timestamp": 1770725509126,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "7jUPxDavcV4F",
    "outputId": "eeafd9e3-7247-44f2-c7f2-ca99abfc0e88"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ce125398f9047238392570cf29e7360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ç¢ºä¿ pipeline æ­£å¸¸é‹ä½œçš„å¸¸è¦‹ä¿®å¾©\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# ==========================================\n",
    "# å»ºç«‹åŸç”Ÿ Transformers Pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,      # æ±ºå®šå›ç­”é•·åº¦\n",
    "    temperature=0.2,         # RAG å»ºè­°è¨­ä½ä¸€é»ï¼Œæ¸›å°‘å¹»è¦º\n",
    "    repetition_penalty=1.1,  # é¿å…é¬¼æ‰“ç‰†\n",
    "    return_full_text=False   # é‡è¦ï¼šè¨­ç‚º False åªå›å‚³ç”Ÿæˆçš„éƒ¨åˆ†\n",
    ")\n",
    "\n",
    "# . è½‰ç‚º LangChain çš„ LLM ç‰©ä»¶\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# (é—œéµ) è½‰ç‚º ChatModel ç‰©ä»¶\n",
    "# ChatHuggingFace æœƒè‡ªå‹•è™•ç† prompt template (User/System/Assistant æ ¼å¼)\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36407,
     "status": "ok",
     "timestamp": 1770725583314,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "mpTIESUCcIAD",
    "outputId": "fb2e1f7f-7c04-425d-f171-6739ec2cd1c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 5.0.0 requires huggingface-hub<2.0,>=1.3.0, but you have huggingface-hub 0.36.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸€å€‹ Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# æ³¨æ„ï¼šé€™è£¡åˆ»æ„ä¸å¯« torchï¼Œç›´æ¥ç”¨ Colab å…§å»ºçš„ï¼Œå°±ä¸æœƒæ‰“æ¶ä¹Ÿä¸æœƒæ…¢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1770725982662,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "S6vhXWnJcRKK"
   },
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval, FaithfulnessMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1770725992502,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "BUz8kypyflnS"
   },
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "# 1. å®šç¾©åŒ…è£å™¨ (Wrapper)\n",
    "class LangChainJudge(DeepEvalBaseLLM):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        # ChatModel (ChatHuggingFace) æ¥æ”¶å­—ä¸²æˆ–è¨Šæ¯åˆ—è¡¨\n",
    "        # ä¸¦å›å‚³ AIMessage ç‰©ä»¶ï¼Œæˆ‘å€‘éœ€è¦å–å®ƒçš„ .content å±¬æ€§\n",
    "        try:\n",
    "            res = self.model.invoke(prompt)\n",
    "            return res.content\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        # DeepEval å…§éƒ¨æœ‰æ™‚æœƒå‘¼å«éåŒæ­¥æ–¹æ³•ï¼Œé€™è£¡ç›´æ¥è½‰çµ¦åŒæ­¥æ–¹æ³•å³å¯\n",
    "        return self.generate(prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Local Qwen Model\"\n",
    "\n",
    "# 2. å¯¦ä¾‹åŒ–ä½ çš„è£åˆ¤\n",
    "# æŠŠå‰é¢æº–å‚™å¥½çš„ chat_model å‚³é€²å»\n",
    "qwen_judge = LangChainJudge(model=chat_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9HoiPZbc2iY"
   },
   "source": [
    "##Test Correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "referenced_widgets": [
      "6d4f3b221b7c4d298271a0d1b2a848d3",
      "026d5f1b04514bd7bf1dad75ff106fea"
     ]
    },
    "executionInfo": {
     "elapsed": 56025,
     "status": "ok",
     "timestamp": 1770726301838,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "PnrkaU5wcTWo",
    "outputId": "e4fed34e-c81c-411b-b998-30cfa1f70b23"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4f3b221b7c4d298271a0d1b2a848d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾—åˆ°çš„åˆ†æ•¸:0.3\n"
     ]
    }
   ],
   "source": [
    "#å®šç¾©è£åˆ¤ (The Judge)\n",
    "correctness_metric=GEval(\n",
    "    name=\"Correctness\",\n",
    "    model=qwen_judge,\n",
    "    evaluation_params=[LLMTestCaseParams.EXPECTED_OUTPUT,LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    evaluation_steps=[\"Determine whether the actual output is factually correct based on the expected output.\"]\n",
    ")\n",
    "\n",
    "#æº–å‚™è€ƒå· (The Test Case)\n",
    "great_answer=\"Madrid is the capital of Spain.\"\n",
    "pred_answer=\"Madrid \"\n",
    "test_case_correctness=LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=great_answer,\n",
    "    actual_output=pred_answer\n",
    ")\n",
    "\n",
    "#é–‹å§‹è©•åˆ† (Grading)\n",
    "\n",
    "correctness_metric.measure(test_case_correctness)\n",
    "print(f\"å¾—åˆ°çš„åˆ†æ•¸:{correctness_metric.score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRmmRJVmc6f3"
   },
   "source": [
    "##Test faithfulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124,
     "referenced_widgets": [
      "c47f05bef5fb423f9f9a3dbfc76a2981",
      "0c8ddd4b26694397b5ad2d66a2e1cc85"
     ]
    },
    "executionInfo": {
     "elapsed": 27485,
     "status": "ok",
     "timestamp": 1770726642542,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "TGMWwtDyeE40",
    "outputId": "634372d0-eb29-41f6-f6c4-7392b2ab37d0"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47f05bef5fb423f9f9a3dbfc76a2981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾—åˆ°çš„åˆ†æ•¸:0.0\n",
      "åŸå› :None\n"
     ]
    }
   ],
   "source": [
    "question=\"what is 3+3?\"\n",
    "context=[\"16\"]\n",
    "generated_answer = \"6\"\n",
    "\n",
    "faithfulness_metric=FaithfulnessMetric(\n",
    "    threshold=0.7,\n",
    "    model=qwen_judge,\n",
    "    include_reason=False\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input = question,\n",
    "    actual_output=generated_answer,\n",
    "    retrieval_context=context\n",
    "\n",
    ")\n",
    "faithfulness_metric.measure(test_case)\n",
    "print(f\"å¾—åˆ°çš„åˆ†æ•¸:{faithfulness_metric.score}\")\n",
    "print(f\"åŸå› :{faithfulness_metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIiBIvKuhc9L"
   },
   "source": [
    "##Test contextual relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141,
     "referenced_widgets": [
      "fcd369e058834504bbc12a60d0d53e02",
      "9229bb8350054da3b1a0c5fea40bfe76"
     ]
    },
    "executionInfo": {
     "elapsed": 115042,
     "status": "ok",
     "timestamp": 1770727853233,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "BFMIL2WpiXsu",
    "outputId": "b99703f1-1d2e-48bf-dacf-f11a60f79254"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcd369e058834504bbc12a60d0d53e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å¾—åˆ°çš„åˆ†æ•¸:0.3333333333333333\n",
      "åŸå› :The score is 0.33 because while the statement 'if the shoes don't fit, then go somewhere else' is partially relevant, the majority of the context contains irrelevant content such as 'this is a test context' and'mike is a cat', which do not connect to the input, limiting overall relevance.\n"
     ]
    }
   ],
   "source": [
    "actual_output = \"then go somewhere else.\"\n",
    "retrieval_context = [\"this is a test context\",\"mike is a cat\",\"if the shoes don't fit, then go somewhere else.\"]\n",
    "gt_answer = \"if the shoes don't fit, then go somewhere else.\"\n",
    "\n",
    "relevance_metric = ContextualRelevancyMetric(\n",
    "    threshold=1,\n",
    "    model=qwen_judge,\n",
    "    include_reason=True\n",
    ")\n",
    "relevance_test_case = LLMTestCase(\n",
    "    input=\"then go somewhere else.\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=retrieval_context,\n",
    "    expected_output=gt_answer\n",
    ")\n",
    "relevance_metric.measure(relevance_test_case)\n",
    "print(f\"å¾—åˆ°çš„åˆ†æ•¸:{relevance_metric.score}\")\n",
    "print(f\"åŸå› :{relevance_metric.reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4usGdAzmSML"
   },
   "source": [
    "##Test two different cases together with several metrics together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1770728463590,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "i11wGZ0tpBeE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. å¢åŠ è¶…æ™‚æ™‚é–“ (åŸæœ¬é è¨­å¯èƒ½åªæœ‰å¹¾åˆ†é˜ï¼Œè¨­ç‚º 1000 ç§’ç¢ºä¿æœ¬åœ°æ¨¡å‹è·‘å¾—å®Œ)\n",
    "os.environ[\"DEEPEVAL_PER_TASK_TIMEOUT_SECONDS_OVERRIDE\"] = \"1000\"\n",
    "\n",
    "# 2. å¦‚æœä½ éœ€è¦å¼·åˆ¶é—œé–‰ä¸¦è¡Œè™•ç† (è®“å®ƒä¸€å€‹ä¸€å€‹è·‘ï¼Œæ¸›è¼•é›»è…¦è² æ“”)\n",
    "# æ³¨æ„ï¼šé€™å–æ±ºæ–¼ DeepEval ç‰ˆæœ¬ï¼Œè‹¥æ­¤ç’°å¢ƒè®Šæ•¸ç„¡æ•ˆï¼Œä¸»è¦ä¾è³´ä¸Šé¢çš„ Timeout è¨­å®š\n",
    "os.environ[\"DEEPEVAL_ASYNC_MODE\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8ed7e8b7b97c48179ec7ba32bba0e6e2",
      "c229bc9c77de4067975bb26092331bcc"
     ]
    },
    "executionInfo": {
     "elapsed": 275714,
     "status": "ok",
     "timestamp": 1770728747257,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "w0yrXOq1nMSj",
    "outputId": "0738a436-eeb2-4d75-c32f-1c5af4cecf4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Local Qwen Model, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Local Qwen Model, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Faithfulness Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Local Qwen Model, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mFaithfulness Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Local Qwen Model, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Contextual Relevancy Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using Local Qwen Model, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span>\n",
       "<span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mContextual Relevancy Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing Local Qwen Model, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\n",
       "\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed7e8b7b97c48179ec7ba32bba0e6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "INFO:deepeval.evaluate.execute:in _a_execute_llm_test_cases\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âŒ Correctness [GEval] (score: 0.4, threshold: 0.5, strict: False, evaluation model: Local Qwen Model, reason: The actual output misses the conditional clause 'if the shoes don't fit' that is present in the expected output, making it factually incomplete. The core logical condition is omitted, so the response does not accurately reflect the intended meaning or structure of the expected output., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: Local Qwen Model, reason: None, error: None)\n",
      "  - âŒ Contextual Relevancy (score: 0.3333333333333333, threshold: 1.0, strict: False, evaluation model: Local Qwen Model, reason: The score is 0.33 because while the statement 'if the shoes don't fit, then go somewhere else' is partially relevant, the majority of the context contains irrelevant content such as 'this is a test context' and'mike is a cat', which do not connect to the input, limiting overall relevance., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: then go somewhere else.\n",
      "  - actual output: then go somewhere else.\n",
      "  - expected output: if the shoes don't fit, then go somewhere else.\n",
      "  - context: None\n",
      "  - retrieval context: ['this is a test context', 'mike is a cat', \"if the shoes don't fit, then go somewhere else.\"]\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - âŒ Correctness [GEval] (score: 0.0, threshold: 0.5, strict: False, evaluation model: Local Qwen Model, reason: The actual output 'MadriD.' is factually incorrect as it does not state that Madrid is the capital of Spain, which is the required factual claim in the expected output. The response fails to convey the correct information even in form, showing a complete mismatch with the expected content., error: None)\n",
      "  - âœ… Faithfulness (score: 1.0, threshold: 0.7, strict: False, evaluation model: Local Qwen Model, reason: None, error: None)\n",
      "  - âœ… Contextual Relevancy (score: 1.0, threshold: 1.0, strict: False, evaluation model: Local Qwen Model, reason: The score is 1.00 because the retrieval context directly contains the statement 'Madrid is the capital of Spain,' which precisely answers the input question, and there are no reasons indicating irrelevance., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is the capital of Spain?\n",
      "  - actual output: MadriD.\n",
      "  - expected output: Madrid is the capital of Spain.\n",
      "  - context: None\n",
      "  - retrieval context: ['Madrid is the capital of Spain.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Correctness [GEval]: 0.00% pass rate\n",
      "Faithfulness: 100.00% pass rate\n",
      "Contextual Relevancy: 50.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">âš  WARNING:</span> No hyperparameters logged.\n",
       "Â» <a href=\"https://deepeval.com/docs/evaluation-prompts\" target=\"_blank\"><span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">Log hyperparameters</span></a> to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;33mâš  WARNING:\u001b[0m No hyperparameters logged.\n",
       "Â» \u001b]8;id=919404;https://deepeval.com/docs/evaluation-prompts\u001b\\\u001b[1;34mLog hyperparameters\u001b[0m\u001b]8;;\u001b\\ to attribute prompts and models to your test runs.\n",
       "\n",
       "================================================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "<span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Evaluation completed ğŸ‰! <span style=\"font-weight: bold\">(</span>time taken: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">275.</span>64s | token cost: <span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span><span style=\"font-weight: bold\">)</span>\n",
       "Â» Test Results <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> total tests<span style=\"font-weight: bold\">)</span>:\n",
       "   Â» Pass Rate: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0</span>% | Passed: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0</span> | Failed: <span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">2</span>\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "Â» Want to share evals with your team, or a place for your test cases to live? â¤ï¸ ğŸ¡\n",
       "  Â» Run <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">'deepeval view'</span> to analyze and save testing results on <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Confident AI</span>.\n",
       "\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n",
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Evaluation completed ğŸ‰! \u001b[1m(\u001b[0mtime taken: \u001b[1;36m275.\u001b[0m64s | token cost: \u001b[3;35mNone\u001b[0m\u001b[1m)\u001b[0m\n",
       "Â» Test Results \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m total tests\u001b[1m)\u001b[0m:\n",
       "   Â» Pass Rate: \u001b[1;36m0.0\u001b[0m% | Passed: \u001b[1;32m0\u001b[0m | Failed: \u001b[1;31m2\u001b[0m\n",
       "\n",
       " ================================================================================ \n",
       "\n",
       "Â» Want to share evals with your team, or a place for your test cases to live? â¤ï¸ ğŸ¡\n",
       "  Â» Run \u001b[1;32m'deepeval view'\u001b[0m to analyze and save testing results on \u001b[38;2;106;0;255mConfident AI\u001b[0m.\n",
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_0', success=False, metrics_data=[MetricData(name='Correctness [GEval]', threshold=0.5, success=False, score=0.4, reason=\"The actual output misses the conditional clause 'if the shoes don't fit' that is present in the expected output, making it factually incomplete. The core logical condition is omitted, so the response does not accurately reflect the intended meaning or structure of the expected output.\", strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Criteria:\\nNone \\n \\nEvaluation Steps:\\n[\\n    \"Determine whether the actual output is factually correct based on the expected output.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.4'), MetricData(name='Faithfulness', threshold=0.7, success=True, score=1.0, reason=None, strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Truths (limit=None):\\n[\\n    \"Mike is a cat.\",\\n    \"If the shoes do not fit, then one should go somewhere else.\"\\n] \\n \\nClaims:\\n[\\n    \"Then go somewhere else.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=1.0, success=False, score=0.3333333333333333, reason=\"The score is 0.33 because while the statement 'if the shoes don't fit, then go somewhere else' is partially relevant, the majority of the context contains irrelevant content such as 'this is a test context' and'mike is a cat', which do not connect to the input, limiting overall relevance.\", strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"this is a test context\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\'this is a test context\\' is not relevant to the input \\'then go somewhere else\\'.\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"mike is a cat\",\\n                \"verdict\": \"no\",\\n                \"reason\": \"The statement \\\\\"mike is a cat\\\\\" is unrelated to the input \\\\\"then go somewhere else.\\\\\"\"\\n            }\\n        ]\\n    },\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"if the shoes don\\'t fit, then go somewhere else\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='then go somewhere else.', actual_output='then go somewhere else.', expected_output=\"if the shoes don't fit, then go somewhere else.\", context=None, retrieval_context=['this is a test context', 'mike is a cat', \"if the shoes don't fit, then go somewhere else.\"], turns=None, additional_metadata=None), TestResult(name='test_case_1', success=False, metrics_data=[MetricData(name='Correctness [GEval]', threshold=0.5, success=False, score=0.0, reason=\"The actual output 'MadriD.' is factually incorrect as it does not state that Madrid is the capital of Spain, which is the required factual claim in the expected output. The response fails to convey the correct information even in form, showing a complete mismatch with the expected content.\", strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Criteria:\\nNone \\n \\nEvaluation Steps:\\n[\\n    \"Determine whether the actual output is factually correct based on the expected output.\"\\n] \\n \\nRubric:\\nNone \\n \\nScore: 0.0'), MetricData(name='Faithfulness', threshold=0.7, success=True, score=1.0, reason=None, strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Truths (limit=None):\\n[\\n    \"Madrid is the capital of Spain.\"\\n] \\n \\nClaims:\\n[\\n    \"MadriD.\"\\n] \\n \\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"idk\",\\n        \"reason\": null\\n    }\\n]'), MetricData(name='Contextual Relevancy', threshold=1.0, success=True, score=1.0, reason=\"The score is 1.00 because the retrieval context directly contains the statement 'Madrid is the capital of Spain,' which precisely answers the input question, and there are no reasons indicating irrelevance.\", strict_mode=False, evaluation_model='Local Qwen Model', error=None, evaluation_cost=None, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdicts\": [\\n            {\\n                \"statement\": \"Madrid is the capital of Spain.\",\\n                \"verdict\": \"yes\",\\n                \"reason\": null\\n            }\\n        ]\\n    }\\n]')], conversational=False, multimodal=False, input='What is the capital of Spain?', actual_output='MadriD.', expected_output='Madrid is the capital of Spain.', context=None, retrieval_context=['Madrid is the capital of Spain.'], turns=None, additional_metadata=None)], confident_link=None, test_run_id=None)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test_case = LLMTestCase(\n",
    "    input=\"What is the capital of Spain?\",\n",
    "    expected_output=\"Madrid is the capital of Spain.\",\n",
    "    actual_output=\"MadriD.\",\n",
    "    retrieval_context=[\"Madrid is the capital of Spain.\"]\n",
    ")\n",
    "#evaluateæœƒè‡ªå‹•åŸ·è¡Œå‰é¢å®šç¾©éçš„\n",
    "evaluate(\n",
    "    test_cases=[relevance_test_case, new_test_case],\n",
    "    metrics=[correctness_metric, faithfulness_metric, relevance_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"ä½ çš„notebookåç¨±.ipynb\"\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "nbformat.write(nb, notebook_path)\n",
    "\n",
    "print(\"Fixed!\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM5gCiBNc08ApikUokDb5T3",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
