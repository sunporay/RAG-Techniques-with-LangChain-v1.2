{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "notebook_path = \"HyPE.ipynb\"\n",
    "\n",
    "nb = nbformat.read(notebook_path, as_version=4)\n",
    "\n",
    "if \"widgets\" in nb.metadata:\n",
    "    del nb.metadata[\"widgets\"]\n",
    "\n",
    "nbformat.write(nb, notebook_path)\n",
    "\n",
    "print(\"Fixed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11853,
     "status": "ok",
     "timestamp": 1770035309140,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "WRgl5T46TPLU",
    "outputId": "474f740c-5638-4945-94b8-7da2de8798ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting langchain-ollama\n",
      "  Downloading langchain_ollama-1.0.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-6.6.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting langchain-huggingface\n",
      "  Downloading langchain_huggingface-1.2.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.2)\n",
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.4.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
      "Downloading langchain_ollama-1.0.1-py3-none-any.whl (29 kB)\n",
      "Downloading pypdf-6.6.2-py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_huggingface-1.2.0-py3-none-any.whl (30 kB)\n",
      "Downloading langchain_experimental-0.4.1-py3-none-any.whl (210 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.1/210.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf, pymupdf, langchain-text-splitters, langchain-ollama, langchain-huggingface, langchain-experimental, langchain-community, faiss-cpu\n",
      "Successfully installed faiss-cpu-1.13.2 langchain-community-0.4.1 langchain-experimental-0.4.1 langchain-huggingface-1.2.0 langchain-ollama-1.0.1 langchain-text-splitters-1.1.0 pymupdf-1.26.7 pypdf-6.6.2\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community langchain-text-splitters langchain-ollama pypdf pymupdf langchain-huggingface sentence-transformers langchain-experimental faiss-cpu  --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 43955,
     "status": "ok",
     "timestamp": 1770035353098,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "zXMAi9wTGehn",
    "outputId": "3b1fedf9-688c-4d84-9cd8-c9806ad75f50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-huggingface 1.2.0 requires huggingface-hub<1.0.0,>=0.33.4, but you have huggingface-hub 1.3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# ç¬¬ä¸€å€‹ Cell\n",
    "!pip install -q bitsandbytes langchain-huggingface sentencepiece\n",
    "!pip install -q -U transformers accelerate\n",
    "# æ³¨æ„ï¼šé€™è£¡åˆ»æ„ä¸å¯« torchï¼Œç›´æ¥ç”¨ Colab å…§å»ºçš„ï¼Œå°±ä¸æœƒæ‰“æ¶ä¹Ÿä¸æœƒæ…¢\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNByr3mwG0fa"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List\n",
    "from concurrent.futures import ThreadPoolExecutor,as_completed\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline, ChatHuggingFace\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531,
     "referenced_widgets": [
      "78291b9a1adb41359d5aa2b17df9be6a",
      "f75b385308654742b694e36b7d6ff9bc",
      "6af340b662664065a75a79968491f164",
      "1618a6e8356448b6acf72353f0304261",
      "544589362a684eeeb76db5af66985d65",
      "731c3ac5752448fb97805532aa8d6150",
      "f892b09816954602a0f1de4a13af1f73",
      "6b6233018ec844559b474838162eca16",
      "bab43e6b094740e68daf1f70e457cce7",
      "56ef95ab487e40618a004831aefdd883",
      "d90a7afaadd64975bad40b88f125f0ec",
      "c5aa3f9a73d94b859cb44148bd5423a7",
      "2fa24872221346a08fbce830bee5883a",
      "45af872e53394408976791d2083c9e11",
      "3654b11a24724ef2859f8a25b1bf5e6c",
      "0e3ae0c9178640dca97a6f4a5a2d4fe4",
      "2c6e267dd7214da59d1a4c81532139c6",
      "35c05aec9de64823bf601f536b154b42",
      "ceb6f4e989b44dbfaf3b79f6f6096461",
      "56df1a6654c9438089566b534ff0295f",
      "505a2d47d3cf4fe593e662aa26b77d3b",
      "b7c78fdeb16348f8ad4c8006125f68e4",
      "0eb8cfb5642549d8a657ef2841d11632",
      "88eef51c58d14d5082db2bc4770bf1f1",
      "985c96e94ab34ee1a673e0e4722e0daf",
      "f80dd9696ef045089d57e8407f032f84",
      "760cde745a5e45bca21f7ac96871fcb0",
      "b42d0a50f7da481c892bf82fbaac74e4",
      "d2bd9b1b82b64fe59a51216577f70dc9",
      "0cde4acfca534161b5cdfe1b84c7476f",
      "de425bce49ff4c16a051e560349245f1",
      "61218a837408406997e58c19c1943691",
      "122a368f10d943af8d45ce8b53a6eb27",
      "44c0d8c04cfe450e91a9bad416fe861a",
      "fea1f09610ee4fcd82fc9b1b506813f0",
      "1514a222c2fa44ef8d06f8a94eb19bb4",
      "c416cce6394f482691b24a37d9fe03d0",
      "0508fa222697454f8eb6d32943d562c6",
      "96b31d15e6b84eb3aab93e5c77ff3193",
      "31a97a45f4be4472bd1b889a85f45466",
      "d78f78f45f2f4e9490d86998b321917b",
      "677e193af0f7430ebacffbc8daf9219a",
      "783b4ccf21024650acf411639ede2e3e",
      "a91507b28eb04017b76a57977bbd99b3",
      "864af7dde93744989edc4ad719a399fd",
      "82e41b0a95a84a9abd8f81d253a7b6ed",
      "63eb349a94d846c1b4aa3d2ba0e7f460",
      "69c6af1ab76b48679a3f81d55771eee3",
      "a1f4b2b373e74dfa959635798f34b384",
      "988572bb60f647ee8454666c906c2266",
      "5ec0fdeb96124616b60411492123b856",
      "4c8a6ed3a3f84c2789defa48638e1be7",
      "12fc11113df44cfaa1ed2a5492c4ed2f",
      "ccf34de583744bb68c3ac0471d65f3c7",
      "6a9a9b5dcf70492e80e37b4bd2dcf720",
      "b51e9689e35e4e8f8d149885e137012a",
      "91069f6508b74f91b47b57e6b2269e6c",
      "58ecc103ee634410afcee743e6a689f0",
      "70e880a8bcd24228b452530848dea0de",
      "af0dc214566c413c86d854a7a8a7c4c2",
      "479953943a63440484312356d96df2aa",
      "3612598277f144afab48c27f06b518ff",
      "fb6481d0917840d7a8f0f2b0ca189d79",
      "81d2c2bf310f4d43b61e449bfd107d92",
      "5579da07fb6a452da6ca5218fc509d35",
      "a96207732ab540e89c7e9c2dd55b64ef",
      "27938e206243498dbb1b54506f892acf",
      "54054b5813164a8e8d11974bb38b922c",
      "d5a926ead03f42eba115fedd79c0c44e",
      "3775e9dc37b042d2a019ab004d5f48bc",
      "5b833371c53f4abb8b62649362dc093d",
      "2d8b35ba879e42f0a0fb887f6b062dc2",
      "7d5d5e897bbc4f0bbc73923acbed1322",
      "bf96aaed0dce4f468f6613d03be4d300",
      "0b7245510f894643b230ed9a6115c797",
      "ef4c2837a22e466480305121db1e6ece",
      "9ab89952b7204c5caa4f293e38cba320",
      "ee06e82d81bc4f92b45e952c523b934e",
      "d7232eff27584d58a72cafc0a51d0b67",
      "db96b9f6e4c540e9b824d134a11c64e1",
      "b67bbd7bf8864d22b78c033cac80ec5c",
      "8867ad3af11b44dfa0a8b699800672bb",
      "c203688649c542caab35183d63413913",
      "ab08705bb4544c5984cc61d734344779",
      "d09d6ae25ae4488ebfe7743c44b3f0a6",
      "1847b2327dc44464b78e6141e14cb4b9",
      "a6a60d2234434252936576eedb6a440e",
      "7633a153a1d048178ffdd6520b4ba897",
      "424bf5ebd71f4d318f577ad22e39addd",
      "23400d58051c463d84c8d0db9625ae34",
      "b5bcd3f0e7cd421b85033e18d254edc4",
      "464e24a929eb4d73abe57290c2fc66ea",
      "b5fbfb5987cd476685fbc94e9dd11058",
      "21cb026edb394b1a8229e1784ab90ac7",
      "bfdccf11cb734e0680484d1fd90b3439",
      "aea7210f6c9c4ac88fdfce056aae8704",
      "8ac50c15a02a4e188373fe2cd50491ae",
      "7a41aa29b1564ef18bcf5f4c05e8960a",
      "cfdc9e9fd79c42a2891a543dd9c996ec",
      "e2c42e81411e4f77bdf22280f575fdeb",
      "32bf03c8db1c41e3b95acfb16083284a",
      "d2a24d1b7d9c4f4b8a40a65cfb0e6fcb",
      "86c7b24a6bf14dcdac164e2b4c0929cf",
      "ea9ed45319d745a3aba63b79f16be3cd",
      "2db21431283942d5b4281d376b3e7019",
      "0248c92914ea460a910ade5967c6e9fe",
      "f11c156baf9648ef8f2bec54d78071d7",
      "e83a31737a8a49eba99853f2d1472885",
      "c701046d824543d497dc1047b0ddbcab",
      "f96f9aa627ff4a32bb723626f6a9bfa7"
     ]
    },
    "executionInfo": {
     "elapsed": 143590,
     "status": "ok",
     "timestamp": 1770035539590,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "Lu5oBiHQGmhH",
    "outputId": "e597f757-7870-4c6e-8ade-a42a348e6cdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78291b9a1adb41359d5aa2b17df9be6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5aa3f9a73d94b859cb44148bd5423a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb8cfb5642549d8a657ef2841d11632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c0d8c04cfe450e91a9bad416fe861a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864af7dde93744989edc4ad719a399fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51e9689e35e4e8f8d149885e137012a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27938e206243498dbb1b54506f892acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee06e82d81bc4f92b45e952c523b934e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "424bf5ebd71f4d318f577ad22e39addd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2c42e81411e4f77bdf22280f575fdeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing `generation_config` together with generation-related arguments=({'temperature', 'repetition_penalty', 'max_new_tokens'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "# ==========================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ç¢ºä¿ pipeline æ­£å¸¸é‹ä½œçš„å¸¸è¦‹ä¿®å¾©\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# ==========================================\n",
    "# å»ºç«‹åŸç”Ÿ Transformers Pipeline\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=128,      # æ±ºå®šå›ç­”é•·åº¦\n",
    "    temperature=0.2,         # RAG å»ºè­°è¨­ä½ä¸€é»ï¼Œæ¸›å°‘å¹»è¦º\n",
    "    repetition_penalty=1.1,  # é¿å…é¬¼æ‰“ç‰†\n",
    "    return_full_text=False   # é‡è¦ï¼šè¨­ç‚º False åªå›å‚³ç”Ÿæˆçš„éƒ¨åˆ†\n",
    ")\n",
    "\n",
    "# . è½‰ç‚º LangChain çš„ LLM ç‰©ä»¶\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# (é—œéµ) è½‰ç‚º ChatModel ç‰©ä»¶\n",
    "# ChatHuggingFace æœƒè‡ªå‹•è™•ç† prompt template (User/System/Assistant æ ¼å¼)\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b80c844233d74abd9def5ba5acbdb55f",
      "49dcd710936443e8ade3f11bcb821f86",
      "686ebe3a1b934f98a87832d5ab877122",
      "10a431ed65144a9f8ff24c143b7ec402",
      "31075b80d6aa4eb7bb4080e0a3c61b70",
      "15c933100b0c472a9df98f9cfe24b982",
      "83141cf454a74f1c9533335040ebbb07",
      "1113602b588e400c9b6a35ccbaea157c",
      "97afd43b1de645b3ad242ab3d63dcc31",
      "43717884d8bd4dccb02b4cf721e7c79f",
      "fd548819fbb24c3f8bf84bd7f712fa66"
     ]
    },
    "executionInfo": {
     "elapsed": 8145,
     "status": "ok",
     "timestamp": 1770035651552,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "QNvzLgoMHH6T",
    "outputId": "0958bdc8-5b62-47b6-c8ea-8667ce82dd60"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b80c844233d74abd9def5ba5acbdb55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfgCptOCG29Z"
   },
   "outputs": [],
   "source": [
    "path=r\"/content/drive/MyDrive/RAG/RAGè³‡æ–™é›†/2401.15884v3.pdf\"\n",
    "Chunk_size=512\n",
    "Chunk_overlap=0\n",
    "\n",
    "question=\"What the CRAG proposed to improve \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTJv61P9G7H-"
   },
   "outputs": [],
   "source": [
    "def retriever_context_per_question(question,query_retriever):\n",
    "\n",
    "    docs=query_retriever.invoke(question,k=2)\n",
    "\n",
    "    context=[doc.page_content for doc in docs]\n",
    "\n",
    "    return context,docs\n",
    "\n",
    "def replace_tab_with_space(texts):\n",
    "    for text in texts:\n",
    "        text.page_content=text.page_content.replace(\"\\t\",\" \")\n",
    "    return texts\n",
    "\n",
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args: context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i + 1}:\")\n",
    "        print(c)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xVHyzo5vG8eC"
   },
   "outputs": [],
   "source": [
    "#ä¸€å€‹chunkç”Ÿæˆå¤šå€‹å•é¡Œï¼Œæ¯å€‹å•é¡Œåšembedding\n",
    "def generate_hypothetical_prompt_embeddings(chunk_text: str):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    chunk_text (str): Text contents of the chunk\n",
    "\n",
    "    Returns:\n",
    "    chunk_text (str): Text contents of the chunk. This is done to make the\n",
    "        multithreading easier\n",
    "    hypothetical prompt embeddings (List[float]): A list of embedding vectors\n",
    "        generated from the questions\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    template=\"\"\"\n",
    "    Analyze the input text and generate essential questions that, when answered,\n",
    "    capture the main points of the text. Each question should be one line,\n",
    "    without numbering or prefixes.\\n\\n\n",
    "    Text:\\n<chunk_text>{chunk_text}</chunk_text>\\n\\n\n",
    "    Questions:\\n\n",
    "    \"\"\"\n",
    "    prompt=PromptTemplate.from_template(template)\n",
    "\n",
    "    question_chain=prompt | llm | StrOutputParser()\n",
    "\n",
    "    question=question_chain.invoke({\"chunk_text\":chunk_text}).replace(\"\\n\\n\",\"\\n\").split(\"\\n\")\n",
    "\n",
    "    return chunk_text,embedding_model.embed_documents(question)\n",
    "\n",
    "#ä¸€å€‹å•é¡Œæœ‰å¤šå€‹embedding vector,æ¯å€‹(å•é¡Œ,vector)éƒ½åŠ å…¥vectorstore\n",
    "\n",
    "\n",
    "def prepare_vectorstore(documents_list):\n",
    "    \"\"\"\n",
    "    ä¿®æ­£å¾Œçš„å‡½æ•¸ï¼Œå°ˆé–€æ¥æ”¶ List[Document]\n",
    "    \"\"\"\n",
    "    # å„²å­˜çµæ§‹ï¼š[(chunk_text, embedding_vector), ...]\n",
    "    text_embedding_pairs = []\n",
    "\n",
    "    # ç”¨ä¾†ä¿ç•™ metadata (ä¾‹å¦‚é ç¢¼)\n",
    "    metadatas = []\n",
    "\n",
    "    print(f\"Processing {len(documents_list)} chunks...\")\n",
    "\n",
    "    for doc in tqdm(documents_list):\n",
    "        try:\n",
    "            # ğŸ›‘ ä¿®æ­£é» 1ï¼šé€™è£¡æ˜¯ Document ç‰©ä»¶ï¼Œå¿…é ˆå–å‡º .page_content æ‰èƒ½ç•¶å­—ä¸²ç”¨\n",
    "            original_text = doc.page_content\n",
    "            original_metadata = doc.metadata  # ä¿ç•™åŸå§‹ metadata\n",
    "\n",
    "            # 1. å‘¼å«ç”Ÿæˆå‡½æ•¸ (å‚³å…¥ç´”å­—ä¸²)\n",
    "            # é€™è£¡å‡è¨­ä½ çš„ generate_hypothetical_prompt_embeddings æ¥æ”¶å­—ä¸²ä¸¦å›å‚³ (text, vectors)\n",
    "            _, question_vectors = generate_hypothetical_prompt_embeddings(original_text)\n",
    "\n",
    "            # 2. è™•ç†å›å‚³çš„å‘é‡\n",
    "            for vec in question_vectors:\n",
    "                # å®‰å…¨æ€§æª¢æŸ¥ï¼šè½‰ç‚º List\n",
    "                if hasattr(vec, 'tolist'):\n",
    "                    vec = vec.tolist()\n",
    "                elif hasattr(vec, 'cpu'):\n",
    "                    vec = vec.detach().cpu().numpy().tolist()\n",
    "\n",
    "                # 3. æ”¶é›†æ•¸æ“šï¼š HyPE æ ¸å¿ƒé‚è¼¯\n",
    "                # è®“ã€Œå•é¡Œå‘é‡ã€å°æ‡‰åˆ°ã€ŒåŸæ–‡å…§å®¹ã€\n",
    "                text_embedding_pairs.append((original_text, vec))\n",
    "                metadatas.append(original_metadata)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing a chunk: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not text_embedding_pairs:\n",
    "        return None\n",
    "\n",
    "    # 4. å»ºç«‹ FAISS ç´¢å¼•\n",
    "    print(f\"Indexing {len(text_embedding_pairs)} vectors...\")\n",
    "\n",
    "    # ğŸ›‘ ä¿®æ­£é» 2ï¼šä½¿ç”¨ from_embeddings\n",
    "    vectorstore = FAISS.from_embeddings(\n",
    "        text_embeddings=text_embedding_pairs,\n",
    "        embedding=embedding_model,\n",
    "        metadatas=metadatas # æŠŠæ­£ç¢ºçš„ metadata å¡å›å»\n",
    "    )\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "def encode_PDF(path,chunk_size,chunk_overlap):\n",
    "    loader=PyPDFLoader(path)\n",
    "    documents=loader.load()\n",
    "\n",
    "    texts_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    texts=texts_splitter.split_documents(documents)\n",
    "\n",
    "    cleaned_texts=replace_tab_with_space(texts)\n",
    "\n",
    "    vectorstore = prepare_vectorstore(cleaned_texts)\n",
    "\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357214,
     "status": "ok",
     "timestamp": 1770036727386,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "xKln943tG--o",
    "outputId": "90eab198-162d-4ccc-8eec-5c32071878e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 41 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/41 [00:00<?, ?it/s]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  2%|â–         | 1/41 [00:08<05:30,  8.27s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  5%|â–         | 2/41 [00:19<06:19,  9.73s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "  7%|â–‹         | 3/41 [00:27<05:55,  9.36s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 10%|â–‰         | 4/41 [00:36<05:30,  8.93s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 12%|â–ˆâ–        | 5/41 [00:44<05:18,  8.85s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 15%|â–ˆâ–        | 6/41 [00:53<05:11,  8.91s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 17%|â–ˆâ–‹        | 7/41 [01:02<04:55,  8.68s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 20%|â–ˆâ–‰        | 8/41 [01:10<04:46,  8.68s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 22%|â–ˆâ–ˆâ–       | 9/41 [01:19<04:40,  8.75s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 24%|â–ˆâ–ˆâ–       | 10/41 [01:27<04:25,  8.55s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 27%|â–ˆâ–ˆâ–‹       | 11/41 [01:36<04:20,  8.70s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 29%|â–ˆâ–ˆâ–‰       | 12/41 [01:45<04:13,  8.73s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 13/41 [01:53<03:59,  8.57s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 14/41 [02:02<03:54,  8.69s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 15/41 [02:11<03:45,  8.66s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 16/41 [02:19<03:34,  8.57s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 17/41 [02:28<03:27,  8.66s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 18/41 [02:37<03:21,  8.76s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 19/41 [02:45<03:08,  8.58s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 20/41 [02:54<03:00,  8.62s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 21/41 [03:03<02:54,  8.73s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 22/41 [03:11<02:42,  8.55s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 23/41 [03:20<02:36,  8.67s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 24/41 [03:29<02:28,  8.76s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 25/41 [03:37<02:16,  8.55s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 26/41 [03:46<02:10,  8.70s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 27/41 [03:55<02:03,  8.80s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 28/41 [04:03<01:52,  8.62s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 29/41 [04:12<01:44,  8.68s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 30/41 [04:21<01:36,  8.75s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 31/41 [04:29<01:25,  8.59s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 32/41 [04:38<01:18,  8.71s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 33/41 [04:47<01:09,  8.71s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 34/41 [04:55<00:59,  8.57s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 35/41 [05:04<00:52,  8.68s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 36/41 [05:13<00:43,  8.80s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 37/41 [05:21<00:34,  8.54s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 38/41 [05:30<00:25,  8.65s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 39/41 [05:39<00:17,  8.76s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 40/41 [05:47<00:08,  8.51s/it]Both `max_new_tokens` (=128) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41/41 [05:56<00:00,  8.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing 292 vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vectorstore=encode_PDF(path,Chunk_size,Chunk_overlap)\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1770036771385,
     "user": {
      "displayName": "314657023æŸç‘",
      "userId": "02820414516176440519"
     },
     "user_tz": -480
    },
    "id": "LhqIUdlpYy4g",
    "outputId": "2638d7c5-b566-485d-80a8-9d4a161a4e5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆåŠŸæª¢ç´¢åˆ° 2 å€‹æ®µè½ï¼š\n",
      "\n",
      "Context 1:\n",
      "instance in practice, as detailed in Table 6. The\n",
      "findings indicate that the self-correction mecha-\n",
      "nism incurs only modest computational overhead\n",
      "while significantly enhancing performance, thereby\n",
      "validating its lightweight nature.\n",
      "6 Conclusion & Limitation\n",
      "This paper studies the problem where RAG-based\n",
      "approaches are challenged if retrieval goes wrong,\n",
      "thereby exposing inaccurate and misleading knowl-\n",
      "edge to generative LMs. Corrective Retrieval\n",
      "Augmented Generation is proposed to improve the\n",
      "robustness of generation. Essentially, a lightweight\n",
      "retrieval evaluator is to estimate and trigger three\n",
      "knowledge retrieval actions discriminately. With\n",
      "the further leverage of web search and optimized\n",
      "knowledge utilization, CRAG has significantly\n",
      "improved the ability of automatic self-correction\n",
      "and efficient utilization of retrieved documents.\n",
      "Experiments extensively demonstrate its adaptabil-\n",
      "ity to RAG-based approaches as well as general-\n",
      "izability across short- and long-form generation\n",
      "tasks. While we primarily proposed to improve the\n",
      "RAG framework from a corrective perspective and\n",
      "CRAG can be seamlessly coupled with various\n",
      "RAG-based approaches, fine-tuning an external\n",
      "retrieval evaluator is inevitable. How to eliminate\n",
      "this external evaluator and equip LLMs with better\n",
      "retrieval evaluation capabilities will be our future\n",
      "work.\n",
      "References\n",
      "Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\n",
      "son, Dmitry Lepikhin, Alexandre Passos, Siamak\n",
      "Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\n",
      "Chen, Eric Chu, Jonathan H. Clark, Laurent El\n",
      "Shafey, Yanping Huang, Kathy Meier-Hellstern,\n",
      "Gaurav Mishra, Erica Moreira, Mark Omernick,\n",
      "Kevin Robinson, Sebastian Ruder, et al. 2023. PaLM\n",
      "2 technical report. CoRR, abs/2305.10403.\n",
      "Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\n",
      "Hannaneh Hajishirzi. 2024. Self-rag: Learning to\n",
      "retrieve, generate, and critique through self-reflection.\n",
      "In The Twelfth International Conference on Learning\n",
      "Representations, ICLR 2024, Vienna, Austria, May\n",
      "7-11, 2024. OpenReview.net.\n",
      "\n",
      "\n",
      "Context 2:\n",
      "Self-RAG (Asai et al., 2024) for demonstrating its\n",
      "adaptability to RAG-based approaches. Results on\n",
      "four datasets of PopQA (Mallen et al., 2023), Biog-\n",
      "raphy (Min et al., 2023), Pub Health (Zhang et al.,\n",
      "2023a), and Arc-Challenge (Bhakthavatsalam et al.,\n",
      "2021) show that CRAG can significantly improve\n",
      "the performance of standard RAG and state-of-the-\n",
      "art Self-RAG, demonstrating its generalizability\n",
      "across both short- and long-form generation tasks.\n",
      "To facilitate others to reproduce our results, we will\n",
      "publish all source code later.\n",
      "In summary, our contributions in this paper are\n",
      "three-fold: 1) This paper studies the scenarios\n",
      "where the retriever returns inaccurate results and,\n",
      "to the best of our knowledge, makes the first\n",
      "attempt to design corrective strategies for RAG to\n",
      "improve its robustness. 2) A plug-and-play method\n",
      "named CRAG is proposed to improve the ability of\n",
      "automatic self-correction and efficient utilization\n",
      "of retrieved documents. 3) Experimental results\n",
      "extensively demonstrate CRAG â€™s adaptability to\n",
      "RAG-based approaches and its generalizability\n",
      "across short- and long-form generation tasks.\n",
      "2 Related Work\n",
      "Hallucinations of LLMs Although LLMs have\n",
      "exhibited impressive abilities to understand instruc-\n",
      "tions and generate fluent language texts (Bang et al.,\n",
      "2023; Qin et al., 2023; Zhong et al., 2023), one of\n",
      "the most severe issues that LLMs have still been\n",
      "struggling with is hallucinations. As many studies\n",
      "found (Tonmoy et al., 2024; Zhang et al., 2023b;\n",
      "Shuster et al., 2021), either outdated information\n",
      "or incorrect knowledge that is activated would\n",
      "seriously result in hallucinations. Large-scale\n",
      "unregulated training data collection, low proportion\n",
      "of high-quality sampling data, imperfection of\n",
      "data allocation in the input space, and many\n",
      "other realistic factors could impact the LLMs and\n",
      "exacerbate the problems. Thus, it is obvious that\n",
      "the lack of accurate and specific knowledge can\n",
      "lead to misleading or even inaccurate generation,\n",
      "which will severely hurt the experience of users in\n",
      "most practical applications.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ä½ çš„ retriever å·²ç¶“å­˜åœ¨è¨˜æ†¶é«”ä¸­äº†ï¼Œä¸éœ€è¦é‡æ–°å»ºç«‹\n",
    "# é‡æ–°åŸ·è¡Œæª¢ç´¢èˆ‡å±•ç¤ºçš„éƒ¨åˆ†å³å¯\n",
    "\n",
    "# 1. é‡æ–°æª¢ç´¢\n",
    "context_list, source_docs = retriever_context_per_question(question, retriever)\n",
    "\n",
    "# 2. ä¿®æ­£å»é‡é‚è¼¯ (é‡å°ç´”æ–‡å­—åˆ—è¡¨å»é‡)\n",
    "unique_context = list(set(context_list))\n",
    "\n",
    "# 3. é¡¯ç¤ºçµæœ\n",
    "print(f\"æˆåŠŸæª¢ç´¢åˆ° {len(unique_context)} å€‹æ®µè½ï¼š\\n\")\n",
    "show_context(unique_context)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMYunC3ZN5zu7vpvmtftYi+",
   "gpuType": "T4",
   "mount_file_id": "1NU6hiRTjSBkjHqmxR7KEyB7GIp-TMBBb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
