{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c973a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\torch-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals to enable efficient handling of complex tasks. This can be done through simple prompting, task-specific instructions, or human input. The goal is to transform big tasks into multiple simpler steps that the LLM can handle more efficiently.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4 #解析HTML\n",
    "\n",
    "from langsmith import Client #導入別人pronmpt模板\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader #發送 HTTP 請求\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "##indexing##\n",
    "\n",
    "#load documents\n",
    "\n",
    "loader=WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\"),\n",
    "\n",
    "    #只抓標題,作者,正文\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    "\n",
    ")\n",
    "\n",
    "docs=loader.load()\n",
    "\n",
    "#spilit\n",
    "\n",
    "#用遞迴式切割，每個 chunk 最多 chunk_size個token先嘗試用較大的語意邊界切（例如：段落 \\n\\n）\n",
    "#相鄰 chunk 會重疊 chunk_overlap 個token避免關鍵資訊被切斷\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300,chunk_overlap=50)\n",
    "splits=text_splitter.split_documents(docs)\n",
    "\n",
    "#embed\n",
    "embedding_model =HuggingFaceEmbeddings(\n",
    "    model_name=\"BAAI/bge-small-zh-v1.5\",\n",
    "\n",
    ")\n",
    "\n",
    "vectorstore=Chroma.from_documents(documents=splits,embedding=embedding_model)\n",
    "retriever=vectorstore.as_retriever()\n",
    "\n",
    "#prompt\n",
    "\n",
    "#調用LangChain Hub上面別人的模板\n",
    "\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\")\n",
    "\n",
    "#LLM\n",
    "\n",
    "llm=ChatOllama(model=\"llama3.2:3b-instruct-q8_0\",temperature=0)\n",
    "\n",
    "# Post-processing 把retriever的page_content訊息拼接好給llm\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"context\": retriever | format_docs,\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is task decomposition for LLM agents?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
